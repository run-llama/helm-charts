## For more information, please refer to the README of this chart or the official LlamaCloud Documentation.
## Ref: https://docs.cloud.llamaindex.ai/

## @section Global Configuration
global:
  ## @param global.cloudProvider Cloud provider where the chart is deployed in.
  ## Supported values: aws, azure, gcp
  cloudProvider: aws

  ## @param global.imagePullSecrets Global Docker registry secret names as an array
  ## imagePullSecrets:
  ##   - name: myRegistryKeySecretName
  ##
  imagePullSecrets: []

  ## @param global.storageClass Storage class to use for dynamic provisioning
  storageClass: ""

  ## Global configuration for all components
  config:
    ## @param global.config.licenseKey License key for all components
    ## @param global.config.existingLicenseKeySecret Name of the secret to use for the license key
    licenseKey: "<input-license-key-here>"
    existingLicenseKeySecret: ""

    ## @param global.config.awsAccessKeyId AWS Access Key ID
    ## @param global.config.awsSecretAccessKey AWS Secret Access Key
    ## @param global.config.existingAwsSecretName Name of the existing secret to use for AWS credentials
    ## If global.cloudProvider is set to "aws", these values are required
    awsAccessKeyId:
    awsSecretAccessKey:
    existingAwsSecretName: ""

    ## @param global.config.postgresql.external.enabled Use an external PostgreSQL database
    ## @param global.config.postgresql.external.host PostgreSQL host
    ## @param global.config.postgresql.external.port PostgreSQL port
    ## @param global.config.postgresql.external.database PostgreSQL database
    ## @param global.config.postgresql.external.username PostgreSQL user
    ## @param global.config.postgresql.external.password PostgreSQL password
    ## @param global.config.postgresql.external.existingSecretName Name of the existing secret to use for PostgreSQL credentials
    postgresql:
      external:
        enabled: false
        host: ""
        port: "5432"
        database: ""
        username: ""
        password: ""
        existingSecretName: ""

    ## @param global.config.mongodb.external.enabled Use an external MongoDB database
    ## @param global.config.mongodb.external.scheme MongoDB connection scheme (i.e. mongodb, mongodb+srv)
    ## @param global.config.mongodb.external.host MongoDB host
    ## @param global.config.mongodb.external.port MongoDB port
    ## @param global.config.mongodb.external.username MongoDB user
    ## @param global.config.mongodb.external.password MongoDB password
    ## @param global.config.mongodb.external.existingSecretName Name of the existing secret to use for MongoDB credentials
    mongodb:
      external:
        enabled: false
        scheme: "mongodb"
        host: ""
        port: "27017"
        username: ""
        password: ""
        existingSecretName: ""

    ## @param global.config.rabbitmq.external.enabled Use an external RabbitMQ instance
    ## @param global.config.rabbitmq.external.scheme RabbitMQ scheme
    ## @param global.config.rabbitmq.external.host RabbitMQ host
    ## @param global.config.rabbitmq.external.port RabbitMQ port
    ## @param global.config.rabbitmq.external.username RabbitMQ user
    ## @param global.config.rabbitmq.external.password RabbitMQ password
    ## @param global.config.rabbitmq.external.connectionString Connection string for the AMQP queue (e.g., for Azure Service Bus)
    ## @param global.config.rabbitmq.external.existingSecretName Name of the existing secret to use for RabbitMQ credentials
    rabbitmq:
      external:
        enabled: false
        scheme: "amqp"
        host: ""
        port: "5672"
        username: ""
        password: ""
        connectionString: ""
        existingSecretName: ""

    ## @param global.config.redis.external.enabled Use an external Redis instance
    ## @param global.config.redis.external.host Redis host
    ## @param global.config.redis.external.port Redis port
    ## @param global.config.redis.external.scheme Redis connection scheme (redis or rediss for SSL)
    ## @param global.config.redis.external.username Redis username (required for Redis 6.0+)
    ## @param global.config.redis.external.password Redis password
    ## @param global.config.redis.external.db Redis database
    ## @param global.config.redis.external.existingSecretName Name of the existing secret to use for Redis credentials
    redis:
      external:
        enabled: false
        host: ""
        port: "6379"
        scheme: "redis"
        username: ""
        password: ""
        db: 0
        existingSecretName: ""

    ## @param global.config.temporal.external.enabled Use an external Temporal instance
    ## @param global.config.temporal.external.host Temporal host
    ## @param global.config.temporal.external.port Temporal port
    temporal:
      external:
        enabled: false
        host: ""
        port: 7233

    ## Uncomment the below lines to configure file store bucket names
    ## The default file store is AWS S3. If you are using a different cloud provider, please set s3proxy.enabled=true
    # parsedDocumentsCloudBucketName: "llama-platform-parsed-documents"
    # parsedEtlCloudBucketName: "llama-platform-etl"
    # parsedExternalComponentsCloudBucketName: "llama-platform-external-components"
    # parsedFileParsingCloudBucketName: "llama-platform-file-parsing"
    # parsedRawFileCloudBucketName: "llama-platform-raw-files"
    # parsedLlamaCloudParseOutputCloudBucketName: "llama-cloud-parse-output"
    # parsedFileScreenshotCloudBucketName: "llama-platform-file-screenshots"
    # llamaExtractOutputCloudBucketName: "llama-platform-extract-output"


## @section Overrides and Common Configuration

## @param nameOverride String to fully override llamacloud.name
##
nameOverride: ""
## @param fullnameOverride String to fully override llamaecloud.fullname
##
fullnameOverride: ""
## @param namespaceOverride String to fully override llamaecloud.namespace
##
namespaceOverride: ""
## @param commonLabels Labels to add to all deployed objects
##
commonLabels: {}
## @param commonAnnotations Annotations to add to all deployed objects
##
commonAnnotations: {}

## @section Ingress Configuration
ingress:
  ## @param ingress.enabled Whether to enable the ingress
  enabled: false

  ## @param ingress.create Whether to create the ingress
  create: true

  ## @param ingress.labels Labels to add to the ingress
  labels: {}

  ## @param ingress.annotations Annotations to add to the ingress
  annotations: {}

  ## @param ingress.host Hostname to use for the ingress
  host:

  ## @param ingress.scheme Scheme to use for the ingress
  scheme: https

  ## @param ingress.tlsSecretName TLS secret name to use for the ingress
  tlsSecretName:

  ## @param ingress.ingressClassName Ingress class name to use for the ingress
  ingressClassName:


## @section LLMs Configuration
llms:
  ## @param llms.enabled Whether to enable the LLMs
  enabled: false

  ## @param llms.openAiApiKey OpenAI API key
  ## @param llms.existingOpenAiApiKeySecretName Name of the existing secret to use for the OpenAI API key
  openAiApiKey: ""
  existingOpenAiApiKeySecretName: ""

  ## @param llms.azureOpenAi.enabled Enable Azure OpenAI for LlamaParse
  ## @param llms.azureOpenAi.existingSecretName Name of the existing secret to use for the Azure OpenAI API key
  ## @param llms.azureOpenAi.deployments Azure OpenAI deployments
  azureOpenAi:
    enabled: false
    existingSecretName: ""
    deployments: []
    # - model: "gpt-4o-mini"
    #   deploymentName: "gpt-4o-mini"
    #   apiKey: ""
    #   baseUrl: "https://api.openai.com/v1"
    #   apiVersion: "2024-08-06"

  ## @param llms.anthropicApiKey Anthropic API key
  ## @param llms.existingAnthropicApiKeySecretName Name of the existing secret to use for the Anthropic API key
  anthropicApiKey: ""
  existingAnthropicApiKeySecretName: ""

  ## @param llms.geminiApiKey Google Gemini API key
  ## @param llms.existingGeminiApiKeySecretName Name of the existing secret to use for the Google Gemini API key
  geminiApiKey: ""
  existingGeminiApiKeySecretName: ""

  ## @param llms.awsBedrock.enabled Enable AWS Bedrock for LlamaParse
  ## @param llms.awsBedrock.existingSecretName Name of the existing secret to use for the AWS Bedrock API key
  ## @param llms.awsBedrock.region AWS Bedrock region
  ## @param llms.awsBedrock.accessKeyId AWS Bedrock access key ID
  ## @param llms.awsBedrock.secretAccessKey AWS Bedrock secret access key
  ## @param llms.awsBedrock.sonnet3_5ModelVersionName Sonnet 3.5 model version name
  ## @param llms.awsBedrock.sonnet3_7ModelVersionName Sonnet 3.7 model version name
  ## @param llms.awsBedrock.sonnet4_0ModelVersionName Sonnet 4.0 model version name
  ## @param llms.awsBedrock.haiku3_5_ModelVersionName Haiku 3.5 model version name
  awsBedrock:
    enabled: false
    existingSecretName: ""
    region: ""
    accessKeyId: ""
    secretAccessKey: ""
    sonnet3_5ModelVersionName: "anthropic.claude-3-5-sonnet-20240620-v1:0"
    sonnet3_7ModelVersionName: "anthropic.claude-3-7-sonnet-20250219-v1:0"
    sonnet4_0ModelVersionName: "anthropic.claude-sonnet-4-20250514-v1:0"
    haiku3_5_ModelVersionName: "anthropic.claude-3-5-haiku-20241022-v1:0"

  ## @param llms.googleVertexAi.enabled Enable Google Vertex AI for LlamaParse
  ## @param llms.googleVertexAi.existingSecretName Name of the existing secret to use for the Google Vertex AI API key
  ## @param llms.googleVertexAi.projectId Google Vertex AI project id
  ## @param llms.googleVertexAi.location Google Vertex AI location
  ## @param llms.googleVertexAi.credentialsJson Google Vertex AI credentials JSON
  googleVertexAi:
    enabled: false
    existingSecretName: ""
    projectId: ""
    location: ""
    credentialsJson: ""

## @section Frontend Configuration
frontend:
  ## @param frontend.name Name suffix of the Frontend related resources
  name: frontend

  ## @param frontend.config.tls.enabled Whether to enable TLS for the Frontend
  ## @param frontend.config.tls.caCertSecretName Name of the secret to use for the CA certificate
  ## @param frontend.config.tls.caCertConfigMapName Name of the config map to use for the CA certificate
  ## @param frontend.config.tls.caCertMountPath Path to mount the CA certificate
  ## @param frontend.config.tls.caCertKey Key of the CA certificate
  config:
    tls:
      enabled: false
      caCertSecretName:
      caCertConfigMapName:
      caCertMountPath: "/etc/llamacloud-ssl/certs"
      caCertKey: "cert.pem"

  ## @param frontend.replicas Number of replicas of Frontend Deployment
  replicas: 1

  ## @param frontend.image.registry Frontend Image registry
  ## @param frontend.image.repository Frontend Image repository
  ## @param frontend.image.tag Frontend Image tag
  ## @param frontend.image.pullPolicy Frontend Image pull policy
  image:
    registry: docker.io
    repository: llamaindex/llamacloud-frontend
    tag: 0.5.9
    pullPolicy: IfNotPresent

  ## @param frontend.service.type Frontend Service type
  ## @param frontend.service.port Frontend Service port
  ## @param frontend.service.labels Labels to add to the service
  ## @param frontend.service.annotations Annotations to add to the service
  service:
    type: ClusterIP
    port: 3000
    labels: {}
    annotations: {}

  ## @param frontend.serviceAccount.create Whether or not to create a new service account
  ## @param frontend.serviceAccount.name Name of the service account
  ## @param frontend.serviceAccount.labels Labels to add to the service account
  ## @param frontend.serviceAccount.annotations Annotations to add to the service account
  ## @param frontend.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
    automountServiceAccountToken: true

  ## @param frontend.labels Labels added to the Frontend Deployment.
  labels: {}
  ## @param frontend.annotations Annotations added to the Frontend Deployment.
  annotations: {}

  ## @param frontend.containerPort Port to expose on the Frontend container
  ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
  containerPort: 3000

  ## @param frontend.extraEnvVariables Extra environment variables to add to Frontend pods
  ## Example:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: BAR
  ##
  extraEnvVariables: []

  ## @param frontend.podAnnotations Annotations to add to the resulting Pods of the Deployment.
  podAnnotations: {}
  ## @param frontend.podLabels Labels to add to the resulting Pods of the Deployment.
  podLabels: {}
  ## @param frontend.podSecurityContext Annotations to add to the resulting Pods of the Deployment.
  podSecurityContext: {}

  ## @param frontend.securityContext Security context for the container
  securityContext: {}
  #  runAsUser: 1000
  #  runAsGroup: 1000

  ## @param frontend.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
  ## To use the recommended values, uncomment the following lines and remove the curly braces after 'resources:'
  #  limits:
  #   cpu: 100m
  #   memory: 128Mi
  #  requests:
  #   cpu: 100m
  #   memory: 128Mi

  ## Frontend Liveness probe configuration
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  livenessProbe:
    httpGet:
      ## @param frontend.livenessProbe.httpGet.path Path to hit for the liveness probe
      ## @param frontend.livenessProbe.httpGet.port Port to hit for the liveness probe
      path: /api/healthz
      port: http

  readinessProbe:
    httpGet:
      ## @param frontend.readinessProbe.httpGet.path Path to hit for the liveness probe
      ## @param frontend.readinessProbe.httpGet.port Port to hit for the liveness probe
      path: /api/healthz
      port: http

  ## forntend.autoscaling HorizontalPodAutoScaler configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  ## The HPA scales by the target CPU utilization percentage by default
  ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
  ## @param frontend.autoscaling.enabled Enable autoscaling for the Frontend Deployment
  ## @param frontend.autoscaling.minReplicas Minimum number of replicas for the Frontend Deployment
  ## @param frontend.autoscaling.maxReplicas Maximum number of replicas for the Frontend Deployment
  ## @param frontend.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the Frontend Deployment
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 4
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  ## @param frontend.volumes List of volumes that can be mounted by containers belonging to the pod
  ## @param frontend.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
  volumes: []
  volumeMounts: []

  ## @param frontend.nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}

  ## @param frontend.tolerations Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## @param frontend.affinity Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}


## @section Backend Configuration
backend:
  ## @param backend.name Name suffix of the Backend related resources
  name: backend

  ## Backend configuration to enable backend features
  config:
    ## @param backend.config.logLevel Log level for the backend
    logLevel: info

    ## @param backend.config.openAiApiKey OpenAI API key
    ## @param backend.config.existingOpenAiApiKeySecretName Name of the existing secret to use for the OpenAI API key
    openAiApiKey: ""
    existingOpenAiApiKeySecretName: ""

    ## Backend Azure OpenAI configuration
    ## @param backend.config.azureOpenAi.enabled Enable Azure OpenAI for backend (Legacy Configuration - Deprecating Soon)
    ## @param backend.config.azureOpenAi.existingSecret Name of the existing secret to use for the Azure OpenAI API key
    ## @param backend.config.azureOpenAi.key Azure OpenAI API key
    ## @param backend.config.azureOpenAi.endpoint Azure OpenAI endpoint
    ## @param backend.config.azureOpenAi.deploymentName Azure OpenAI deployment
    ## @param backend.config.azureOpenAi.apiVersion Azure OpenAI API version
    azureOpenAi:
      ## This is a legacy configuration that will be deprecated in the near future.
      enabled: false
      existingSecret: ""
      key: ""
      endpoint: ""
      deploymentName: ""
      apiVersion: ""

    ## Backend Basic Auth configuration
    ## @param backend.config.basicAuth.enabled Enable Basic Auth for the backend
    ## @param backend.config.basicAuth.validEmailDomain Valid email domain for the application
    ## @param backend.config.basicAuth.jwtSecret JWT secret for the backend
    ## @param backend.config.basicAuth.existingSecretName Name of the existing secret to use for the JWT secret
    basicAuth:
      enabled: false
      validEmailDomain:
      jwtSecret: "documentparsingisfun1!"
      existingSecretName:

    ## Backend OpenID Connect configuration
    ## @param backend.config.oidc.existingSecretName Name of the existing secret to use for OIDC configuration
    ## @param backend.config.oidc.discoveryUrl OIDC discovery URL
    ## @param backend.config.oidc.clientId OIDC client ID
    ## @param backend.config.oidc.clientSecret OIDC client secret
    oidc:
      existingSecretName: ""
      discoveryUrl: ""
      clientId: ""
      clientSecret: ""

    ## Backend QDRANT Data-Sink configuration
    ## @param backend.config.qdrant.enabled Enable QDRANT Data-Sink for backend
    ## @param backend.config.qdrant.existingSecret Name of the existing secret to use for the QDRANT Data-Sink
    ## @param backend.config.qdrant.url QDRANT Data-Sink host
    ## @param backend.config.qdrant.apiKey QDRANT Data-Sink API key
    qdrant:
      enabled: false
      existingSecret: ""
      url: ""
      apiKey: ""

    ## @param backend.config.llamaExtractMultimodalModel LlamaExtract multimodal model (gemini-2.0-flash, gemini-2.5-pro, openai-gpt-4-1)
    llamaExtractMultimodalModel: "gemini-2.0-flash"
    ## @param backend.config.llamaExtractSchemaGenerationModel LlamaExtract schema generation model (gemini-2.0-flash, openai-gpt-4-1-mini)
    llamaExtractSchemaGenerationModel: "gemini-2.0-flash"
    ## @param backend.config.llamaExtractMaxPages LlamaExtract max pages allowed
    llamaExtractMaxPages: 500
    ## @param backend.config.llamaExtractMaxFileSizeMb LlamaExtract max file size (MB) allowed
    llamaExtractMaxFileSizeMb: 100
    ## @param backend.config.llamaExtractMaxFileSizeUiMb LlamaExtract max file size (MB) allowed for UI
    llamaExtractMaxFileSizeUiMb: 30

  ## @param backend.replicas Number of replicas of Backend Deployment
  replicas: 1

  ## Backend Image information
  ## @param backend.image.registry Backend Image registry
  ## @param backend.image.repository Backend Image repository
  ## @param backend.image.tag Backend Image tag
  ## @param backend.image.pullPolicy Backend Image pull policy
  image:
    registry: docker.io
    repository: llamaindex/llamacloud-backend
    tag: 0.5.9
    pullPolicy: IfNotPresent

  ## Backend Service information
  ## @param backend.service.type Backend Service type
  ## @param backend.service.port Backend Service port
  ## @param backend.service.labels Labels to add to the service
  ## @param backend.service.annotations Annotations to add to the service
  service:
    type: ClusterIP
    port: 8000
    labels: {}
    annotations: {}

  serviceAccount:
    ## @param backend.serviceAccount.create Whether or not to create a new service account
    ## @param backend.serviceAccount.name Name of the service account
    ## @param backend.serviceAccount.labels Labels to add to the service account
    ## @param backend.serviceAccount.annotations Annotations to add to the service account
    ## @param backend.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
    create: true
    name: ""
    labels: {}
    annotations: {}
    automountServiceAccountToken: true

  ## @param backend.labels Labels added to the Backend Deployment.
  labels: {}
  ## @param backend.annotations Annotations added to the Backend Deployment.
  annotations: {}

  ## @param backend.containerPort Port to expose on the Backend container
  ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
  containerPort: 8000

  ## @param backend.extraEnvVariables Extra environment variables to add to backend pods
  ## Example:
  ## extraEnvVariables:
  ##   - name: FOO
  ##     value: BAR
  ##   - name: OPENAI_API_KEY
  ##     valueFrom:
  ##       secretKeyRef:
  ##         name: openai-api-secret
  ##         key: openai-api-key
  extraEnvVariables: []

  ## Backend External secrets to load environment variables from
  externalSecrets:
    ## @param backend.externalSecrets.enabled Enable external secrets for the Backend Deployment
    enabled: false
    ## @param backend.externalSecrets.secrets List of external secrets to load environment variables from
    ## ["secret-1", "secret-2"]
    secrets: []

  ## @param backend.podAnnotations Annotations to add to the resulting Pods of the Deployment.
  podAnnotations: {}

  ## @param backend.podLabels Labels to add to the resulting Pods of the Deployment.
  podLabels: {}

  ## @param backend.podSecurityContext Pod security context
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  podSecurityContext: {}

  ## @param backend.securityContext Security context for the container
  securityContext: {}
  #  runAsUser: 1000
  #  runAsGroup: 1000

  ## @param backend.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
    # requests:
    #   memory: 4Gi
    #   cpu: 500m
    # limits:
    #   memory: 6Gi
    #   cpu: 2

  ## Backend Liveness probe configuration
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  livenessProbe:
    ## @param backend.livenessProbe.httpGet.path Path to hit for the liveness probe
    ## @param backend.livenessProbe.httpGet.port Port to hit for the liveness probe
    ## @param backend.livenessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param backend.livenessProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param backend.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param backend.livenessProbe.initialDelaySeconds Number of seconds after the container has started before the probe is initiated
    failureThreshold: 30
    initialDelaySeconds: 30
    httpGet:
      path: /api/health
      port: 8000
    periodSeconds: 10
    timeoutSeconds: 5

  ## Backend Readiness probe configuration
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  readinessProbe:
    ## @param backend.readinessProbe.httpGet.path Path to hit for the readiness probe
    ## @param backend.readinessProbe.httpGet.port Port to hit for the readiness probe
    ## @param backend.readinessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param backend.readinessProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param backend.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param backend.readinessProbe.initialDelaySeconds Number of seconds after the container has started before the probe is initiated
    failureThreshold: 30
    initialDelaySeconds: 30
    httpGet:
      path: /api/health
      port: 8000
    periodSeconds: 10
    timeoutSeconds: 5

  ## Backend Startup probe configuration
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  startupProbe:
    ## @param backend.startupProbe.httpGet.path Path to hit for the startup probe
    ## @param backend.startupProbe.httpGet.port Port to hit for the startup probe
    ## @param backend.startupProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param backend.startupProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param backend.startupProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param backend.startupProbe.initialDelaySeconds Number of seconds after the container has started before the probe is initiated
    failureThreshold: 30
    initialDelaySeconds: 30
    httpGet:
      path: /api/health
      port: 8000
    periodSeconds: 15
    timeoutSeconds: 5

  ## Backend HorizontalPodAutoScaler configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  ## The HPA scales by the target CPU utilization percentage by default
  ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
  autoscaling:
    ## @param backend.autoscaling.enabled Enable autoscaling for the Backend Deployment
    ## @param backend.autoscaling.minReplicas Minimum number of replicas for the Backend Deployment
    ## @param backend.autoscaling.maxReplicas Maximum number of replicas for the Backend Deployment
    ## @param backend.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the Backend Deployment
    enabled: true
    minReplicas: 1
    maxReplicas: 8
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  ## Backend PodDisruptionBudget configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  podDisruptionBudget:
    ## @param backend.podDisruptionBudget.enabled Enable PodDisruptionBudget for the Backend Deployment
    ## @param backend.podDisruptionBudget.maxUnavailable Maximum number of pods that can be unavailable during an update
    enabled: false
    maxUnavailable: 1

  ## @param backend.volumes List of volumes that can be mounted by containers belonging to the pod
  ## @param backend.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
  volumes: []
  volumeMounts: []

  ## @param backend.nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}

  ## @param backend.tolerations Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## @param backend.affinity Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}

  metrics:
    ## @param backend.metrics.enabled Enable metrics for the backend
    enabled: false
    serviceMonitor:
      ## @param backend.metrics.serviceMonitor.enabled Enable service monitor for the backend
      enabled: false
      ## @param backend.metrics.serviceMonitor.selector Selector for the service monitor
      selector: {}
      ## @param backend.metrics.serviceMonitor.additionalLabels Additional labels for the service monitor
      additionalLabels: {}
      ## @param backend.metrics.serviceMonitor.annotations Annotations for the service monitor
      annotations: {}
      ## @param backend.metrics.serviceMonitor.interval Interval for the service monitor
      interval: 30s
      ## @param backend.metrics.serviceMonitor.scrapeTimeout Timeout for the service monitor
      scrapeTimeout: 15s
      ## @param backend.metrics.serviceMonitor.relabelings Relabelings for the service monitor
      relabelings: []
      ## @param backend.metrics.serviceMonitor.metricRelabelings Metric relabelings for the service monitor
      metricRelabelings: []
      ## @param backend.metrics.serviceMonitor.scheme Scheme for the service monitor
      scheme: http
      ## @param backend.metrics.serviceMonitor.tlsConfig TLS configuration for the service monitor
      tlsConfig: {}

    rules:
      ## @param backend.metrics.rules.enabled Enable prometheus rules for the backend services
      enabled: false
      ## @param backend.metrics.rules.namespace Namespace for the rules
      namespace: ""
      ## @param backend.metrics.rules.selector Selector for the rules
      selector: {}
      ## @param backend.metrics.rules.additionalLabels Additional labels for the rules
      additionalLabels: {}
      ## @param backend.metrics.rules.annotations Annotations for the rules
      annotations: {}
      ## @param backend.metrics.rules.spec Rules for the backend
      spec: []
      # - alert: JobErrorRateTooHigh
      #   expr: |
      #     sum(rate(llamacloud_job_status_counter_total{status="ERROR"}[5m])) by (job_name) > 0.10
      #   for: 5m
      #   labels:
      #     severity: error
      #   annotations:
      #     summary: "[LlamaCloud] Job error rate too high"
      #     description: >
      #       The job has an error rate of {{ $value }} which is too high.


## @section JobsService Configuration
jobsService:
  ## @param jobsService.name Name suffix of the JobsService related resources
  name: jobs-service

  config:
    ## @param jobsService.config.logLevel Log level for the JobsService
    logLevel: "info"

  ## @param jobsService.replicas Number of replicas of JobsService Deployment
  replicas: 1

  ## JobsService Image information
  ## @param jobsService.image.registry JobsService Image registry
  ## @param jobsService.image.repository JobsService Image repository
  ## @param jobsService.image.tag JobsService Image tag
  ## @param jobsService.image.pullPolicy JobsService Image pull policy
  image:
    registry: docker.io
    repository: llamaindex/llamacloud-jobs-service
    tag: 0.5.9
    pullPolicy: IfNotPresent

  ## JobsService Service information\
  ## @param jobsService.service.type JobsService Service type
  ## @param jobsService.service.port JobsService Service port
  service:
    type: ClusterIP
    port: 8002

  ## JobsService ServiceAccount configuration
  ## @param jobsService.serviceAccount.create Whether or not to create a new service account
  ## @param jobsService.serviceAccount.name Name of the service account
  ## @param jobsService.serviceAccount.labels Labels to add to the service account
  ## @param jobsService.serviceAccount.annotations Annotations to add to the service account
  ## @param jobsService.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
    automountServiceAccountToken: true

  ## @param jobsService.containerPort Port to expose on the JobsService container
  ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
  containerPort: 8002

  ## @param jobsService.extraEnvVariables Extra environment variables to add to jobsService pods
  ## Example:
  ## extraEnvVariables:
  ##   - name: FOO
  ##     value: BAR
  ##   - name: OPENAI_API_KEY
  ##     valueFrom:
  ##       secretKeyRef:
  ##         name: openai-api-secret
  ##         key: openai-api-key
  extraEnvVariables: []

  ## Jobs Service External secrets to load environment variables from
  externalSecrets:
    ## @param jobsService.externalSecrets.enabled Enable external secrets for the JobsService Deployment
    enabled: false
    ## @param jobsService.externalSecrets.secrets List of external secrets to load environment variables from
    ## ["secret-1", "secret-2"]
    secrets: []

  ## @param jobsService.podAnnotations Annotations to add to the resulting Pods of the Deployment.
  podAnnotations: {}

  ## @param jobsService.podLabels Labels to add to the resulting Pods of the Deployment.
  podLabels: {}

  ## @param jobsService.podSecurityContext Pod security context
  podSecurityContext: {}

  ## @param jobsService.securityContext Security context for the container
  securityContext: {}
  #  runAsUser: 1000
  #  runAsGroup: 1000

  ## @param jobsService.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
    # To use the recommended values, uncomment the following lines and remove the curly braces after 'resources:'
    # requests:
    #   memory: 4Gi
    #   cpu: 500m
    # limits:
    #   memory: 8Gi
    #   cpu: 2

  ## Job Service Liveness probe configuration
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  livenessProbe:
    ## @param jobsService.livenessProbe.httpGet.path Path to hit for the liveness probe
    ## @param jobsService.livenessProbe.httpGet.port Port to hit for the liveness prob
    ## @param jobsService.livenessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param jobsService.livenessProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param jobsService.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    failureThreshold: 30
    httpGet:
      path: /api/health
      port: 8002
    periodSeconds: 15
    timeoutSeconds: 10

  ## Job Service Readiness probe configuration
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  readinessProbe:
    ## @param jobsService.readinessProbe.httpGet.path Path to hit for the liveness probe
    ## @param jobsService.readinessProbe.httpGet.port Port to hit for the liveness probe
    ## @param jobsService.readinessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param jobsService.readinessProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param jobsService.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    failureThreshold: 30
    httpGet:
      path: /api/health
      port: 8002
    periodSeconds: 15
    timeoutSeconds: 10

  ## Job Service Startup probe configuration
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  startupProbe:
    ## @param jobsService.startupProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param jobsService.startupProbe.httpGet.path Path to hit for the readiness probe
    ## @param jobsService.startupProbe.httpGet.port Port to hit for the readiness probe
    ## @param jobsService.startupProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param jobsService.startupProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param jobsService.startupProbe.initialDelaySeconds Number of seconds after the container has started before the probe is initiated
    failureThreshold: 30
    initialDelaySeconds: 30
    httpGet:
      path: /api/health
      port: 8002
    periodSeconds: 15
    timeoutSeconds: 10

  ## Job Service HorizontalPodAutoScaler configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  ## The HPA scales by the target CPU utilization percentage by default
  ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
  ## @param jobsService.autoscaling.enabled Enable autoscaling for the JobsService Deployment
  ## @param jobsService.autoscaling.minReplicas Minimum number of replicas for the JobsService Deployment
  ## @param jobsService.autoscaling.maxReplicas Maximum number of replicas for the JobsService Deployment
  ## @param jobsService.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the JobsService Deployment
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 4
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  ## @param jobsService.volumes List of volumes that can be mounted by containers belonging to the pod
  ## @param jobsService.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
  volumes: []
  volumeMounts: []

  ## @param jobsService.nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}

  ## @param jobsService.tolerations Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## @param jobsService.affinity Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}

  metrics:
    ## @param jobsService.metrics.enabled Enable metrics for the jobsService
    enabled: false
    serviceMonitor:
      ## @param jobsService.metrics.serviceMonitor.enabled Enable service monitor for the jobsService
      enabled: false
      ## @param jobsService.metrics.serviceMonitor.selector Selector for the service monitor
      selector: {}
      ## @param jobsService.metrics.serviceMonitor.additionalLabels Additional labels for the service monitor
      additionalLabels: {}
      ## @param jobsService.metrics.serviceMonitor.annotations Annotations for the service monitor
      annotations: {}
      ## @param jobsService.metrics.serviceMonitor.interval Interval for the service monitor
      interval: 30s
      ## @param jobsService.metrics.serviceMonitor.scrapeTimeout Timeout for the service monitor
      scrapeTimeout: 15s
      ## @param jobsService.metrics.serviceMonitor.relabelings Relabelings for the service monitor
      relabelings: []
      ## @param jobsService.metrics.serviceMonitor.metricRelabelings Metric relabelings for the service monitor
      metricRelabelings: []
      ## @param jobsService.metrics.serviceMonitor.scheme Scheme for the service monitor
      scheme: http
      ## @param jobsService.metrics.serviceMonitor.tlsConfig TLS configuration for the service monitor
      tlsConfig: {}

## @section JobsWorker Configuration
jobsWorker:
  ## @param jobsWorker.name Name suffix of the JobsWorker related resources
  name: jobs-worker

  config:
    ## @param jobsWorker.config.logLevel Log level for the JobsWorker
    logLevel: "info"

    ## @param jobsWorker.config.maxJobsInExecutionPerJobType Maximum number of jobs in execution per job type
    ## @param jobsWorker.config.maxIndexJobsInExecution Maximum number of index jobs in execution
    ## @param jobsWorker.config.maxDocumentIngestionJobsInExecution Maximum number of document ingestion jobs in execution
    ## @param jobsWorker.config.includeJobErrorDetails Whether to always include job error details in API and the UI
    ## @param jobsWorker.config.defaultTransformDocumentTimeoutSeconds Default timeout in seconds for document transformation jobs
    ## @param jobsWorker.config.transformEmbeddingCharLimit Character limit for transform embedding operations
    maxJobsInExecutionPerJobType: 10
    maxIndexJobsInExecution: 0
    maxDocumentIngestionJobsInExecution: 1
    includeJobErrorDetails: true
    defaultTransformDocumentTimeoutSeconds: "240"
    transformEmbeddingCharLimit: "11520000"

  ## @param jobsWorker.replicas Number of replicas of JobsWorker Deployment
  replicas: 1

  ## JobsWorker Image information
  ## @param jobsWorker.image.registry JobsWorker Image registry
  ## @param jobsWorker.image.repository JobsWorker Image repository
  ## @param jobsWorker.image.tag JobsWorker Image tag
  ## @param jobsWorker.image.pullPolicy JobsWorker Image pull policy
  image:
    registry: docker.io
    repository: llamaindex/llamacloud-jobs-worker
    tag: 0.5.9
    pullPolicy: IfNotPresent

  ## JobsWorker Service information
  ## @param jobsWorker.service.type JobsWorker Service type
  ## @param jobsWorker.service.port JobsWorker Service port
  service:
    type: ClusterIP
    port: 8001

  ## ServiceAccount configuration
  ## @param jobsWorker.serviceAccount.create Whether or not to create a new service account
  ## @param jobsWorker.serviceAccount.name Name of the service account
  ## @param jobsWorker.serviceAccount.labels Labels to add to the service account
  ## @param jobsWorker.serviceAccount.annotations Annotations to add to the service account
  ## @param jobsWorker.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
    automountServiceAccountToken: true

  ## @param jobsWorker.labels Labels added to the JobsWorker Deployment.
  labels: {}
  ## @param jobsWorker.annotations Annotations added to the JobsWorker Deployment.
  annotations: {}

  ## @param jobsWorker.containerPort Port to expose on the jobsWorker container
  ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
  containerPort: 8001

  ## @param jobsWorker.extraEnvVariables Extra environment variables to add to jobsWorker pods
  ## Example:
  ## extraEnvVariables:
  ##   - name: FOO
  ##     value: BAR
  ##   - name: OPENAI_API_KEY
  ##     valueFrom:
  ##       secretKeyRef:
  ##         name: openai-api-secret
  ##         key: openai-api-key
  extraEnvVariables: []

  ## External secrets to load environment variables from
  ## @param jobsWorker.externalSecrets.enabled Enable external secrets for the JobsWorker Deployment
  ## @param jobsWorker.externalSecrets.secrets List of external secrets to load environment variables from
  externalSecrets:
    ## @param jobsWorker.externalSecrets.enabled Enable external secrets for the JobsWorker Deployment
    enabled: false
    ## @param jobsWorker.externalSecrets.secrets List of external secrets to load environment variables from
    ## ["secret-1", "secret-2"]
    secrets: []

  ## @param jobsWorker.podAnnotations Annotations to add to the resulting Pods of the Deployment.
  podAnnotations: {}

  ## @param jobsWorker.podLabels Labels to add to the resulting Pods of the Deployment.
  podLabels: {}

  ## @param jobsWorker.podSecurityContext Pod security context
  podSecurityContext: {}

  ## @param jobsWorker.securityContext Security context for the container
  securityContext: {}
  #  runAsUser: 1000
  #  runAsGroup: 1000

  ## @param jobsWorker.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
    # To use the recommended values, uncomment the following lines and remove the curly braces after 'resources:'
    # resources:
    #   requests:
    #     memory: 4Gi
    #     cpu: 500m
    #   limits:
    #     memory: 6Gi
    #     cpu: 2

  ## Jobs Worker Liveness probe configuration
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  livenessProbe:
    ## @param jobsWorker.livenessProbe.httpGet.path Path to hit for the liveness probe
    ## @param jobsWorker.livenessProbe.httpGet.port Port to hit for the liveness probe
    ## @param jobsWorker.livenessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param jobsWorker.livenessProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param jobsWorker.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param jobsWorker.livenessProbe.initialDelaySeconds Number of seconds after the container has started before the probe is initiated
    failureThreshold: 30
    initialDelaySeconds: 30
    httpGet:
      path: /api/health
      port: 8001
    periodSeconds: 15
    timeoutSeconds: 5

  ## Jobs Worker Readiness probe configuration
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  readinessProbe:
    ## @param jobsWorker.readinessProbe.httpGet.path Path to hit for the liveness probe
    ## @param jobsWorker.readinessProbe.httpGet.port Port to hit for the liveness probe
    ## @param jobsWorker.readinessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param jobsWorker.readinessProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param jobsWorker.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param jobsWorker.readinessProbe.initialDelaySeconds Number of seconds after the container has started before the probe is initiated
    failureThreshold: 30
    initialDelaySeconds: 30
    httpGet:
      path: /api/health
      port: 8001
    periodSeconds: 15
    timeoutSeconds: 5

  ## Jobs Worker Startup probe configuration
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  startupProbe:
    ## @param jobsWorker.startupProbe.httpGet.path Path to hit for the liveness probe
    ## @param jobsWorker.startupProbe.httpGet.port Port to hit for the liveness probe
    ## @param jobsWorker.startupProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param jobsWorker.startupProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param jobsWorker.startupProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param jobsWorker.startupProbe.initialDelaySeconds Number of seconds after the container has started before the probe is initiated
    failureThreshold: 30
    initialDelaySeconds: 30
    httpGet:
      path: /api/health
      port: 8001
    periodSeconds: 15
    timeoutSeconds: 5

  ## Jobs Worker HorizontalPodAutoScaler configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  ## The HPA scales by the target CPU utilization percentage by default
  ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
  autoscaling:
    ## @param jobsWorker.autoscaling.enabled Enable autoscaling for the JobsWorker Deployment
    ## @param jobsWorker.autoscaling.minReplicas Minimum number of replicas for the JobsWorker Deployment
    ## @param jobsWorker.autoscaling.maxReplicas Maximum number of replicas for the JobsWorker Deployment
    ## @param jobsWorker.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the JobsWorker Deployment
    enabled: true
    minReplicas: 1
    maxReplicas: 4
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  ## @param jobsWorker.volumes List of volumes that can be mounted by containers belonging to the pod
  ## @param jobsWorker.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
  volumes: []
  volumeMounts: []

  ## @param jobsWorker.nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}

  ## @param jobsWorker.tolerations Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## @param jobsWorker.affinity Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}

  metrics:
    ## @param jobsWorker.metrics.enabled Enable metrics for the jobsWorker
    enabled: false
    serviceMonitor:
      ## @param jobsWorker.metrics.serviceMonitor.enabled Enable service monitor for the jobsWorker
      enabled: false
      ## @param jobsWorker.metrics.serviceMonitor.selector Selector for the service monitor
      selector: {}
      ## @param jobsWorker.metrics.serviceMonitor.additionalLabels Additional labels for the service monitor
      additionalLabels: {}
      ## @param jobsWorker.metrics.serviceMonitor.annotations Annotations for the service monitor
      annotations: {}
      ## @param jobsWorker.metrics.serviceMonitor.interval Interval for the service monitor
      interval: 30s
      ## @param jobsWorker.metrics.serviceMonitor.scrapeTimeout Timeout for the service monitor
      scrapeTimeout: 15s
      ## @param jobsWorker.metrics.serviceMonitor.relabelings Relabelings for the service monitor
      relabelings: []
      ## @param jobsWorker.metrics.serviceMonitor.metricRelabelings Metric relabelings for the service monitor
      metricRelabelings: []
      ## @param jobsWorker.metrics.serviceMonitor.scheme Scheme for the service monitor
      scheme: http
      ## @param jobsWorker.metrics.serviceMonitor.tlsConfig TLS configuration for the service monitor
      tlsConfig: {}

## @section LlamaParse Configuration
llamaParse:
  ## @param llamaParse.name Name suffix of the LlamaParse related resources
  name: llamaparse

  config:
    ## @param llamaParse.config.debugMode Enable debug mode for LlamaParse
    debugMode: false
    ## @param llamaParse.config.maxQueueConcurrency Max number of jobs the worker can process at the same time
    maxQueueConcurrency: 3

    ## @param llamaParse.config.openAiApiKey OpenAI API key
    ## @param llamaParse.config.existingOpenAiApiKeySecretName Name of the existing secret to use for the OpenAI API key
    openAiApiKey: ""
    existingOpenAiApiKeySecretName: ""

    ## @param llamaParse.config.azureOpenAi.enabled Enable Azure OpenAI for LlamaParse
    ## @param llamaParse.config.azureOpenAi.existingSecret Name of the existing secret to use for the Azure OpenAI API key
    ## @param llamaParse.config.azureOpenAi.key Azure OpenAI API key
    ## @param llamaParse.config.azureOpenAi.endpoint Azure OpenAI endpoint
    ## @param llamaParse.config.azureOpenAi.deploymentName Azure OpenAI deployment
    ## @param llamaParse.config.azureOpenAi.apiVersion Azure OpenAI API version
    azureOpenAi:
      enabled: false
      # Note each model has its own deployment name / api version / key on azure openai
      # For more information, please refer to https://docs.cloud.llamaindex.ai/self_hosting/configuration/azure-openai
      existingSecret: ""
      key: ""
      endpoint: ""
      deploymentName: ""
      apiVersion: ""

    ## @param llamaParse.config.anthropicApiKey Anthropic API key
    ## @param llamaParse.config.existingAnthropicApiKeySecret Name of the existing secret to use for the Anthropic API key
    anthropicApiKey: ""
    existingAnthropicApiKeySecret: ""

    ## @param llamaParse.config.geminiApiKey Google Gemini API key
    ## @param llamaParse.config.existingGeminiApiKeySecret Name of the existing secret to use for the Google Gemini API key
    geminiApiKey: ""
    existingGeminiApiKeySecret: ""

    ## @param llamaParse.config.awsBedrock.enabled Enable AWS Bedrock for LlamaParse
    ## @param llamaParse.config.awsBedrock.existingSecret Name of the existing secret to use for the AWS Bedrock API key
    ## @param llamaParse.config.awsBedrock.region AWS Bedrock region
    ## @param llamaParse.config.awsBedrock.accessKeyId AWS Bedrock access key ID
    ## @param llamaParse.config.awsBedrock.secretAccessKey AWS Bedrock secret access key
    ## @param llamaParse.config.awsBedrock.sonnet3_5ModelVersionName Sonnet 3.5 model version name
    ## @param llamaParse.config.awsBedrock.sonnet3_7ModelVersionName Sonnet 3.7 model version name
    ## @param llamaParse.config.awsBedrock.sonnet4_0ModelVersionName Sonnet 4.0 model version name
    ## @param llamaParse.config.awsBedrock.haiku3_5_ModelVersionName Haiku 3.5 model version name
    awsBedrock:
      enabled: false
      existingSecret: ""
      region: ""
      accessKeyId: ""
      secretAccessKey: ""
      sonnet3_5ModelVersionName: "anthropic.claude-3-5-sonnet-20240620-v1:0"
      sonnet3_7ModelVersionName: "anthropic.claude-3-7-sonnet-20250219-v1:0"
      sonnet4_0ModelVersionName: "anthropic.claude-sonnet-4-20250514-v1:0"
      haiku3_5_ModelVersionName: "anthropic.claude-3-5-haiku-20241022-v1:0"

    ## @param llamaParse.config.googleVertexAi.enabled Enable Google Vertex AI for LlamaParse
    ## @param llamaParse.config.googleVertexAi.existingSecret Name of the existing secret to use for the Google Vertex AI API key
    ## @param llamaParse.config.googleVertexAi.projectId Google Vertex AI project id
    ## @param llamaParse.config.googleVertexAi.location Google Vertex AI location
    ## @param llamaParse.config.googleVertexAi.credentialsJson Google Vertex AI credentials JSON
    googleVertexAi:
      enabled: false
      existingSecret: ""
      projectId: ""
      location: ""
      credentialsJson: ""

    ## @param llamaParse.config.s3UploadBucket S3 bucket to upload files to
    ## @param llamaParse.config.s3OutputBucket S3 bucket to output files to
    s3UploadBucket: "llama-platform-file-parsing"
    s3OutputBucket: "llama-platform-file-parsing"

  ## @param llamaParse.replicas Number of replicas of LlamaParse Deployment
  replicas: 2

  ## LlamaParse Image information
  ## @param llamaParse.image.registry LlamaParse Image registry
  ## @param llamaParse.image.repository LlamaParse Image repository
  ## @param llamaParse.image.tag LlamaParse Image tag
  ## @param llamaParse.image.pullPolicy LlamaParse Image pull policy
  image:
    registry: docker.io
    repository: llamaindex/llamacloud-llamaparse
    tag: 0.5.9
    pullPolicy: IfNotPresent

  ## ServiceAccount configuration
  ## @param llamaParse.serviceAccount.create Whether or not to create a new service account
  ## @param llamaParse.serviceAccount.name Name of the service account
  ## @param llamaParse.serviceAccount.labels Labels to add to the service account
  ## @param llamaParse.serviceAccount.annotations Annotations to add to the service account
  ## @param llamaParse.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
    automountServiceAccountToken: true

  ## @param llamaParse.labels Labels added to the LlamaParse Deployment.
  labels: {}
  ## @param llamaParse.annotations Annotations added to the LlamaParse Deployment.
  annotations: {}

  ## @param llamaParse.containerPort Port to expose on the LlamaParse container
  ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
  containerPort: 8004

  ## @param llamaParse.service.type LlamaParse Service type
  ## @param llamaParse.service.port LlamaParse Service port
  service:
    type: ClusterIP
    port: 8004

  ## @param llamaParse.extraEnvVariables Extra environment variables to add to llamaParse pods
  ## Example:
  ## extraEnvVariables:
  ##   - name: FOO
  ##     value: BAR
  ##   - name: OPENAI_API_KEY
  ##     valueFrom:
  ##       secretKeyRef:
  ##         name: openai-api-secret
  ##         key: openai-api-key
  extraEnvVariables: []

  ## External Secrets Configuration
  ## @param llamaParse.externalSecrets.enabled Enable external secrets for the LlamaParse Deployment
  ## @param llamaParse.externalSecrets.secrets List of external secrets to load environment variables from
  externalSecrets:
    ## @param llamaParse.externalSecrets.enabled Enable external secrets for the LlamaParse Deployment
    enabled: false
    ## @param llamaParse.externalSecrets.secrets List of external secrets to load environment variables from
    ## ["secret-1", "secret-2"]
    secrets: []

  ## @param llamaParse.podAnnotations Annotations to add to the resulting Pods of the Deployment.
  podAnnotations: {}

  ## @param llamaParse.podLabels Labels to add to the resulting Pods of the Deployment.
  podLabels: {}

  ## @param llamaParse.podSecurityContext Pod security context
  podSecurityContext: {}

  ## @param llamaParse.securityContext Security context for the container
  securityContext: {}
  #  runAsUser: 1000
  #  runAsGroup: 1000

  ## LlamaParse resources: Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  ## Below are the recommended values for LlamaParse. Please adjust according to your deployment's needs.
  resources:
    ## @param llamaParse.resources.requests.memory Memory request for the LlamaParse container
    ## @param llamaParse.resources.requests.cpu CPU request for the LlamaParse container
    ## @param llamaParse.resources.limits.memory Memory limit for the LlamaParse container
    ## @param llamaParse.resources.limits.cpu CPU limit for the LlamaParse container
    requests:
      memory: 6Gi
      cpu: 3
    limits:
      memory: 13Gi
      cpu: 7

  ## LlamaParse Liveness probe configuration
  livenessProbe:
    ## @param llamaParse.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param llamaParse.livenessProbe.httpGet.path Path to hit for the liveness probe
    ## @param llamaParse.livenessProbe.httpGet.port Port to hit for the liveness probe
    ## @param llamaParse.livenessProbe.initialDelaySeconds Number of seconds after the container has started before liveness probes are initiated
    ## @param llamaParse.livenessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param llamaParse.livenessProbe.timeoutSeconds Number of seconds after which the probe times out
    failureThreshold: 10
    httpGet:
      path: /livez
      port: 8004
    initialDelaySeconds: 30
    periodSeconds: 30
    timeoutSeconds: 5

  ## LlamaParse Readiness probe configuration
  readinessProbe:
    ## @param llamaParse.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param llamaParse.readinessProbe.httpGet.path Path to hit for the readiness probe
    ## @param llamaParse.readinessProbe.httpGet.port Port to hit for the readiness probe
    ## @param llamaParse.readinessProbe.initialDelaySeconds Number of seconds after the container has started before readiness probes are initiated
    ## @param llamaParse.readinessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param llamaParse.readinessProbe.timeoutSeconds Number of seconds after which the probe times out
    failureThreshold: 10
    httpGet:
      path: /readyz
      port: 8004
    initialDelaySeconds: 30
    periodSeconds: 30
    timeoutSeconds: 5

  ## LlamaParse HorizontalPodAutoScaler configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  ## The HPA scales by the target CPU utilization percentage by default
  ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
  autoscaling:
    ## @param llamaParse.autoscaling.enabled Enable autoscaling for the LlamaParse Deployment
    ## @param llamaParse.autoscaling.minReplicas Minimum number of replicas for the LlamaParse Deployment
    ## @param llamaParse.autoscaling.maxReplicas Maximum number of replicas for the LlamaParse Deployment
    ## @param llamaParse.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the LlamaParse Deployment
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  ## KEDA configuration for the LlamaParse Deployment
  ## Ref: https://keda.sh
  ## Note: .Values.llamaParse.autoscaling.enabled must be false for KEDA to be enabled
  keda:
    ## @param llamaParse.keda.enabled Enable KEDA for the llamaParse Deployment
    enabled: false
    ## @param llamaParse.keda.additionalAnnotations Additional annotations for the KEDA
    additionalAnnotations: {}
    ## @param llamaParse.keda.additionalLabels Additional labels for the KEDA
    additionalLabels: {}
    ## @param llamaParse.keda.pollingInterval Polling interval for the KEDA
    pollingInterval: 15
    ## @param llamaParse.keda.cooldownPeriod Cooldown period for the KEDA
    cooldownPeriod: 120
    ## @param llamaParse.keda.minReplicaCount Minimum number of replicas for the KEDA
    minReplicaCount: 2
    ## @param llamaParse.keda.maxReplicaCount Maximum number of replicas for the KEDA
    maxReplicaCount: 10
    ## @param llamaParse.keda.initialCooldownPeriod Initial cooldown period for the KEDA
    initialCooldownPeriod: 0
    ## @param llamaParse.keda.fallback Fallback for the KEDA
    fallback: {}
    ## @param llamaParse.keda.advanced Advanced configuration for the KEDA
    advanced: {}
    ## @param llamaParse.keda.triggers Triggers for the KEDA
    ## This cannot be empty if .Values.llamaParse.keda.enabled is true
    triggers: []

  ## PodDisruptionBudget configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  podDisruptionBudget:
    ## @param llamaParse.podDisruptionBudget.enabled Enable PodDisruptionBudget for the LlamaParse Deployment
    ## @param llamaParse.podDisruptionBudget.maxUnavailable Maximum number of unavailable pods
    enabled: true
    maxUnavailable: 1

  ## @param llamaParse.volumes List of volumes that can be mounted by containers belonging to the pod
  ## @param llamaParse.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
  volumes: []
  volumeMounts: []

  ## @param llamaParse.nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}

  ## @param llamaParse.tolerations Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## @param llamaParse.affinity Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}

  metrics:
    ## @param llamaParse.metrics.enabled Enable metrics for the llamaParse
    enabled: false
    serviceMonitor:
      ## @param llamaParse.metrics.serviceMonitor.enabled Enable service monitor for the llamaParse
      enabled: false
      ## @param llamaParse.metrics.serviceMonitor.selector Selector for the service monitor
      selector: {}
      ## @param llamaParse.metrics.serviceMonitor.additionalLabels Additional labels for the service monitor
      additionalLabels: {}
      ## @param llamaParse.metrics.serviceMonitor.annotations Annotations for the service monitor
      annotations: {}
      ## @param llamaParse.metrics.serviceMonitor.interval Interval for the service monitor
      interval: 30s
      ## @param llamaParse.metrics.serviceMonitor.scrapeTimeout Timeout for the service monitor
      scrapeTimeout: 15s
      ## @param llamaParse.metrics.serviceMonitor.relabelings Relabelings for the service monitor
      relabelings: []
      ## @param llamaParse.metrics.serviceMonitor.metricRelabelings Metric relabelings for the service monitor
      metricRelabelings: []
      ## @param llamaParse.metrics.serviceMonitor.scheme Scheme for the service monitor
      scheme: http
      ## @param llamaParse.metrics.serviceMonitor.tlsConfig TLS configuration for the service monitor
      tlsConfig: {}

    rules:
      ## @param llamaParse.metrics.rules.enabled Enable rules for the llamaParse
      enabled: false
      ## @param llamaParse.metrics.rules.namespace Namespace for the rules
      namespace: ""
      ## @param llamaParse.metrics.rules.selector Selector for the rules
      selector: {}
      ## @param llamaParse.metrics.rules.additionalLabels Additional labels for the rules
      additionalLabels: {}
      ## @param llamaParse.metrics.rules.annotations Annotations for the rules
      annotations: {}
      ## @param llamaParse.metrics.rules.spec Rules for the llamaParse
      spec: []
      # - alert: ParsingErrorRate
      #   expr: |
      #     sum(rate(llamaparse_done_total{status="error"}[5m])) > 0.05
      #   for: 5m
      #   labels:
      #     severity: error
      #   annotations:
      #     summary: "[LlamaParse] Parsing error rate too high"
      #     description: >
      #       Parsing has an error rate of {{ $value }} which is too high.

## @section LlamaParseOcr Configuration
llamaParseOcr:
  ## @param llamaParseOcr.enabled Enable LlamaParseOcr
  enabled: true
  ## @param llamaParseOcr.name Name suffix of the LlamaParseOcr related resources
  name: llamaparse-ocr

  ## @param llamaParseOcr.replicas Number of replicas of LlamaParseOcr Deployment
  replicas: 2

  ## GPU configuration for OCR processing
  gpu:
    ## @param llamaParseOcr.gpu.enabled Enable GPU acceleration for OCR processing (if false, uses CPU backend)
    enabled: false

  ## LlamaParseOcr Image information
  ## @param llamaParseOcr.image.registry LlamaParseOcr Image registry
  ## @param llamaParseOcr.image.repository LlamaParseOcr Image repository
  ## @param llamaParseOcr.image.tag LlamaParseOcr Image tag
  ## @param llamaParseOcr.image.pullPolicy LlamaParseOcr Image pull policy
  image:
    registry: docker.io
    repository: llamaindex/llamacloud-llamaparse-ocr
    tag: 0.5.9
    pullPolicy: IfNotPresent

  ## LlamaParseOcr Service information
  ## @param llamaParseOcr.service.type LlamaParseOcr Service type
  ## @param llamaParseOcr.service.port LlamaParseOcr Service port
  service:
    type: ClusterIP
    port: 8080

  ## ServiceAccount configuration
  ## @param llamaParseOcr.serviceAccount.create Whether or not to create a new service account
  ## @param llamaParseOcr.serviceAccount.name Name of the service account
  ## @param llamaParseOcr.serviceAccount.labels Labels to add to the service account
  ## @param llamaParseOcr.serviceAccount.annotations Annotations to add to the service account
  ## @param llamaParseOcr.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
    automountServiceAccountToken: true

  ## @param llamaParseOcr.containerPort Port to expose on the LlamaParseOcr container
  ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
  containerPort: 8080

  ## @param llamaParseOcr.labels Labels added to the LlamaParseOcr Deployment.
  labels: {}
  ## @param llamaParseOcr.annotations Annotations added to the LlamaParseOcr Deployment.
  annotations: {}

  ## @param llamaParseOcr.podAnnotations Annotations to add to the resulting Pods of the Deployment.
  podAnnotations: {}

  ## @param llamaParseOcr.podLabels Labels to add to the resulting Pods of the Deployment.
  podLabels: {}

  ## @param llamaParseOcr.podSecurityContext Pod security context
  podSecurityContext: {}

  ## @param llamaParseOcr.securityContext Security context for the container
  securityContext: {}
  #  runAsUser: 1000
  #  runAsGroup: 1000

  ## @param llamaParseOcr.extraEnvVariables Extra environment variables to add to llamaParseOcr pods
  ## Example:
  ## extraEnvVariables:
  ##   - name: FOO
  ##     value: BAR
  extraEnvVariables: []

  ## LlamaParseOcr Resources: Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  ## Below are the recommended settings for a basic deployment. Adjust according to your needs.
  resources:
    ## @param llamaParseOcr.resources.requests.memory Memory request for the LlamaParse container
    ## @param llamaParseOcr.resources.requests.cpu CPU request for the LlamaParse container
    ## @param llamaParseOcr.resources.limits.memory Memory limit for the LlamaParse container
    ## @param llamaParseOcr.resources.limits.cpu CPU limit for the LlamaParse container
    requests:
      cpu: 2
      memory: 12Gi
    limits:
      cpu: 4
      memory: 16Gi

  ## LlamaParse Ocr Liveness probe configuration
  livenessProbe:
    ## @param llamaParseOcr.livenessProbe.httpGet.path Path to hit for the liveness probe
    ## @param llamaParseOcr.livenessProbe.httpGet.port Port to hit for the liveness probe
    ## @param llamaParseOcr.livenessProbe.httpGet.scheme Scheme to use for the liveness probe
    ## @param llamaParseOcr.livenessProbe.initialDelaySeconds Number of seconds after the container has started before liveness probes are initiated
    ## @param llamaParseOcr.livenessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param llamaParseOcr.livenessProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param llamaParseOcr.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param llamaParseOcr.livenessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed
    failureThreshold: 5
    httpGet:
      path: /health_check
      port: 8080
      scheme: HTTP
    initialDelaySeconds: 10
    periodSeconds: 15
    successThreshold: 1
    timeoutSeconds: 5

  ## LlamaParse Ocr Readiness probe configuration
  readinessProbe:
    ## @param llamaParseOcr.readinessProbe.httpGet.path Path to hit for the readiness probe
    ## @param llamaParseOcr.readinessProbe.httpGet.port Port to hit for the readiness probe
    ## @param llamaParseOcr.readinessProbe.httpGet.scheme Scheme to use for the readiness probe
    ## @param llamaParseOcr.readinessProbe.initialDelaySeconds Number of seconds after the container has started before readiness probes are initiated
    ## @param llamaParseOcr.readinessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param llamaParseOcr.readinessProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param llamaParseOcr.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param llamaParseOcr.readinessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed
    failureThreshold: 5
    httpGet:
      path: /health_check
      port: 8080
      scheme: HTTP
    initialDelaySeconds: 10
    periodSeconds: 15
    successThreshold: 1
    timeoutSeconds: 5

  ## HorizontalPodAutoScaler configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  ## The HPA scales by the target CPU utilization percentage by default
  ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
  autoscaling:
    ## @param llamaParseOcr.autoscaling.enabled Enable autoscaling for the LlamaParseOcr Deployment
    ## @param llamaParseOcr.autoscaling.minReplicas Minimum number of replicas for the LlamaParseOcr Deployment
    ## @param llamaParseOcr.autoscaling.maxReplicas Maximum number of replicas for the LlamaParseOcr Deployment
    ## @param llamaParseOcr.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the LlamaParseOcr Deployment
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  ## KEDA configuration for the LlamaParseOcr Deployment
  ## Ref: https://keda.sh
  ## Note: .Values.llamaParseOcr.autoscaling.enabled must be false for KEDA to be enabled
  keda:
    ## @param llamaParseOcr.keda.enabled Enable KEDA for the LlamaParseOcr Deployment
    enabled: false
    ## @param llamaParseOcr.keda.additionalAnnotations Additional annotations for the KEDA
    additionalAnnotations: {}
    ## @param llamaParseOcr.keda.additionalLabels Additional labels for the KEDA
    additionalLabels: {}
    ## @param llamaParseOcr.keda.pollingInterval Polling interval for the KEDA
    pollingInterval: 15
    ## @param llamaParseOcr.keda.cooldownPeriod Cooldown period for the KEDA
    cooldownPeriod: 120
    ## @param llamaParseOcr.keda.minReplicaCount Minimum number of replicas for the KEDA
    minReplicaCount: 2
    ## @param llamaParseOcr.keda.maxReplicaCount Maximum number of replicas for the KEDA
    maxReplicaCount: 10
    ## @param llamaParseOcr.keda.initialCooldownPeriod Initial cooldown period for the KEDA
    initialCooldownPeriod: 0
    ## @param llamaParseOcr.keda.fallback Fallback for the KEDA
    fallback: {}
    ## @param llamaParseOcr.keda.advanced Advanced configuration for the KEDA
    advanced: {}
    ## @param llamaParseOcr.keda.triggers Triggers for the KEDA
    ## This cannot be empty if .Values.llamaParseOcr.keda.enabled is true
    triggers: []

  ## PodDisruptionBudget configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  podDisruptionBudget:
    ## @param llamaParseOcr.podDisruptionBudget.enabled Enable PodDisruptionBudget for the LlamaParseOcr Deployment
    ## @param llamaParseOcr.podDisruptionBudget.maxUnavailable Maximum number of unavailable pods
    enabled: true
    maxUnavailable: 1

  ## @param llamaParseOcr.volumes List of volumes that can be mounted by containers belonging to the pod
  ## @param llamaParseOcr.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
  volumes: []
  volumeMounts: []

  ## @param llamaParseOcr.nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}

  ## @param llamaParseOcr.tolerations Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## @param llamaParseOcr.affinity Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}

  metrics:
    ## @param llamaParseOcr.metrics.enabled Enable metrics for the llamaParseOcr
    enabled: false
    serviceMonitor:
      ## @param llamaParseOcr.metrics.serviceMonitor.enabled Enable service monitor for the llamaParseOcr
      enabled: false
      ## @param llamaParseOcr.metrics.serviceMonitor.selector Selector for the service monitor
      selector: {}
      ## @param llamaParseOcr.metrics.serviceMonitor.additionalLabels Additional labels for the service monitor
      additionalLabels: {}
      ## @param llamaParseOcr.metrics.serviceMonitor.annotations Annotations for the service monitor
      annotations: {}
      ## @param llamaParseOcr.metrics.serviceMonitor.interval Interval for the service monitor
      interval: 30s
      ## @param llamaParseOcr.metrics.serviceMonitor.scrapeTimeout Timeout for the service monitor
      scrapeTimeout: 15s
      ## @param llamaParseOcr.metrics.serviceMonitor.relabelings Relabelings for the service monitor
      relabelings: []
      ## @param llamaParseOcr.metrics.serviceMonitor.metricRelabelings Metric relabelings for the service monitor
      metricRelabelings: []
      ## @param llamaParseOcr.metrics.serviceMonitor.scheme Scheme for the service monitor
      scheme: http
      ## @param llamaParseOcr.metrics.serviceMonitor.tlsConfig TLS configuration for the service monitor
      tlsConfig: {}

## @section LlamaParse Layout Detection API Configuration
llamaParseLayoutDetectionApi:
  ## @param llamaParseLayoutDetectionApi.enabled Enable LlamaParseLayoutDetectionApi
  enabled: false

  ## @param llamaParseLayoutDetectionApi.name Name suffix of the LlamaParse Layout Detection Api related resources
  name: llamaparse-layout-detection-api

  ## @param llamaParseLayoutDetectionApi.replicas Number of replicas of LlamaParse Layout Detection Api Deployment
  replicas: 1

  ## @param llamaParseLayoutDetectionApi.config.logLevel Log level for the LlamaParse Layout Detection Api
  config:
    logLevel: INFO

  ## LlamaParse Layout Detection Api Image information
  ## @param llamaParseLayoutDetectionApi.image.registry LlamaParse Layout Detection Api Image registry
  ## @param llamaParseLayoutDetectionApi.image.repository LlamaParse Layout Detection Api Image repository
  ## @param llamaParseLayoutDetectionApi.image.tag LlamaParse Layout Detection Api Image tag
  ## @param llamaParseLayoutDetectionApi.image.pullPolicy LlamaParse Layout Detection Api Image pull policy
  image:
    registry: docker.io
    repository: llamaindex/llamacloud-layout-detection-api
    tag: 0.5.9
    pullPolicy: IfNotPresent

  ## LlamaParse Layout Detection Api Service information
  ## @param llamaParseLayoutDetectionApi.service.type LlamaParse Layout Detection Api Service type
  ## @param llamaParseLayoutDetectionApi.service.port LlamaParse Layout Detection Api Service port
  service:
    type: ClusterIP
    port: 8000

  ## ServiceAccount configuration
  ## @param llamaParseLayoutDetectionApi.serviceAccount.create Whether or not to create a new service account
  ## @param llamaParseLayoutDetectionApi.serviceAccount.name Name of the service account
  ## @param llamaParseLayoutDetectionApi.serviceAccount.labels Labels to add to the service account
  ## @param llamaParseLayoutDetectionApi.serviceAccount.annotations Annotations to add to the service account
  ## @param llamaParseLayoutDetectionApi.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
    automountServiceAccountToken: true

  ## @param llamaParseLayoutDetectionApi.containerPort Port to expose on the LlamaParse Layout Detection Api container
  ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
  containerPort: 8000

  ## @param llamaParseLayoutDetectionApi.labels Labels added to the LlamaParse Layout Detection Api Deployment.
  labels: {}
  ## @param llamaParseLayoutDetectionApi.annotations Annotations added to the LlamaParse Layout Detection Api Deployment.
  annotations: {}

  ## @param llamaParseLayoutDetectionApi.podAnnotations Annotations to add to the resulting Pods of the Deployment.
  podAnnotations: {}

  ## @param llamaParseLayoutDetectionApi.podLabels Labels to add to the resulting Pods of the Deployment.
  podLabels: {}

  ## @param llamaParseLayoutDetectionApi.podSecurityContext Pod security context
  podSecurityContext: {}

  ## @param llamaParseLayoutDetectionApi.securityContext Security context for the container
  securityContext: {}
  #  runAsUser: 1000
  #  runAsGroup: 1000

  ## @param llamaParseLayoutDetectionApi.extraEnvVariables Extra environment variables to add to LlamaParse Layout Detection Api pods
  ## Example:
  ## extraEnvVariables:
  ##   - name: FOO
  ##     value: BAR
  extraEnvVariables: []

  ## LlamaParse Layout Detection Api Resources: Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  ## Below are the recommended settings for a basic deployment. Adjust according to your needs.
  resources:
    ## @param llamaParseLayoutDetectionApi.resources.requests.memory Memory request for the LlamaParse Layout Detection API Container
    ## @param llamaParseLayoutDetectionApi.resources.requests.cpu CPU request for the LlamaParse Layout Detection API Container
    ## @param llamaParseLayoutDetectionApi.resources.limits.memory Memory limit for the LlamaParse Layout Detection API Container
    ## @param llamaParseLayoutDetectionApi.resources.limits.cpu CPU limit for the LlamaParse Layout Detection API Container
    requests:
      cpu: 1
      memory: 6Gi
    limits:
      cpu: 2
      memory: 12Gi

  ## LlamaParse Layout Detection API Liveness probe configuration
  livenessProbe:
    ## @param llamaParseLayoutDetectionApi.livenessProbe.httpGet.path Path to hit for the liveness probe
    ## @param llamaParseLayoutDetectionApi.livenessProbe.httpGet.port Port to hit for the liveness probe
    ## @param llamaParseLayoutDetectionApi.livenessProbe.httpGet.scheme Scheme to use for the liveness probe
    ## @param llamaParseLayoutDetectionApi.livenessProbe.initialDelaySeconds Number of seconds after the container has started before liveness probes are initiated
    ## @param llamaParseLayoutDetectionApi.livenessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param llamaParseLayoutDetectionApi.livenessProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param llamaParseLayoutDetectionApi.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param llamaParseLayoutDetectionApi.livenessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed
    failureThreshold: 5
    httpGet:
      path: /health
      port: 8000
      scheme: HTTP
    initialDelaySeconds: 10
    periodSeconds: 15
    successThreshold: 1
    timeoutSeconds: 5

  ## LlamaParse Layout Detection API Readiness probe configuration
  readinessProbe:
    ## @param llamaParseLayoutDetectionApi.readinessProbe.httpGet.path Path to hit for the readiness probe
    ## @param llamaParseLayoutDetectionApi.readinessProbe.httpGet.port Port to hit for the readiness probe
    ## @param llamaParseLayoutDetectionApi.readinessProbe.httpGet.scheme Scheme to use for the readiness probe
    ## @param llamaParseLayoutDetectionApi.readinessProbe.initialDelaySeconds Number of seconds after the container has started before readiness probes are initiated
    ## @param llamaParseLayoutDetectionApi.readinessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param llamaParseLayoutDetectionApi.readinessProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param llamaParseLayoutDetectionApi.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param llamaParseLayoutDetectionApi.readinessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed
    failureThreshold: 5
    httpGet:
      path: /health
      port: 8000
      scheme: HTTP
    initialDelaySeconds: 10
    periodSeconds: 15
    successThreshold: 1
    timeoutSeconds: 5

  ## HorizontalPodAutoScaler configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  ## The HPA scales by the target CPU utilization percentage by default
  ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
  autoscaling:
    ## @param llamaParseLayoutDetectionApi.autoscaling.enabled Enable autoscaling for the LlamaParse Layout Detection Api Deployment
    ## @param llamaParseLayoutDetectionApi.autoscaling.minReplicas Minimum number of replicas for the LlamaParse Layout Detection Api Deployment
    ## @param llamaParseLayoutDetectionApi.autoscaling.maxReplicas Maximum number of replicas for the LlamaParse Layout Detection Api Deployment
    ## @param llamaParseLayoutDetectionApi.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the LlamaParse Layout Detection Api Deployment
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  ## PodDisruptionBudget configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  podDisruptionBudget:
    ## @param llamaParseLayoutDetectionApi.podDisruptionBudget.enabled Enable PodDisruptionBudget for the LlamaParse Layout Detection Api Deployment
    ## @param llamaParseLayoutDetectionApi.podDisruptionBudget.maxUnavailable Maximum number of unavailable pods
    enabled: true
    maxUnavailable: 1

  ## @param llamaParseLayoutDetectionApi.volumes List of volumes that can be mounted by containers belonging to the pod
  ## @param llamaParseLayoutDetectionApi.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
  volumes: []
  volumeMounts: []

  ## @param llamaParseLayoutDetectionApi.nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}

  ## @param llamaParseLayoutDetectionApi.tolerations Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## @param llamaParseLayoutDetectionApi.affinity Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}

## @section Usage Configuration
usage:
  ## @param usage.name Name suffix of the usage related resources
  name: usage

  ## @param usage.replicas Number of replicas of usage Deployment
  replicas: 1

  ## Usage Image information
  ## @param usage.image.registry Usage Image registry
  ## @param usage.image.repository Usage Image repository
  ## @param usage.image.tag Usage Image tag
  ## @param usage.image.pullPolicy Usage Image pull policy
  image:
    registry: docker.io
    repository: llamaindex/llamacloud-usage
    tag: 0.5.9
    pullPolicy: IfNotPresent

  ## Usage Service information
  ## @param usage.service.type Usage Service type
  ## @param usage.service.port Usage Service port
  service:
    type: ClusterIP
    port: 8005

  ## ServiceAccount configuration
  ## @param usage.serviceAccount.create Whether or not to create a new service account
  ## @param usage.serviceAccount.name Name of the service account
  ## @param usage.serviceAccount.labels Labels to add to the service account
  ## @param usage.serviceAccount.annotations Annotations to add to the service account
  ## @param usage.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
    automountServiceAccountToken: true

  ## @param usage.containerPort Port to expose on the usage container
  ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
  containerPort: 8005

  ## @param usage.labels Labels added to the usage Deployment.
  labels: {}
  ## @param usage.annotations Annotations added to the usage Deployment.
  annotations: {}

  ## @param usage.extraEnvVariables Extra environment variables to add to usage pods
  ## Example:
  ## extraEnvVariables:
  ##   - name: FOO
  ##     value: BAR
  ##   - name: OPENAI_API_KEY
  ##     valueFrom:
  ##       secretKeyRef:
  ##         name: openai-api-secret
  ##         key: openai-api-key
  extraEnvVariables: []

  ## Usage Service External Secrets
  externalSecrets:
    ## @param usage.externalSecrets.enabled Enable external secrets for the Usage Deployment
    enabled: false
    ## @param usage.externalSecrets.secrets List of external secrets to load environment variables from
    ## ["secret-1", "secret-2"]
    secrets: []

  ## @param usage.podAnnotations Annotations to add to the resulting Pods of the Deployment.
  podAnnotations: {}

  ## @param usage.podLabels Labels to add to the resulting Pods of the Deployment.
  podLabels: {}

  ## @param usage.podSecurityContext Pod security context
  podSecurityContext: {}

  ## @param usage.securityContext Security context for the container
  securityContext: {}
  #  runAsUser: 1000
  #  runAsGroup: 1000

  ## @param usage.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
    ## To use the recommended values, uncomment the following lines and remove the curly braces after 'resources:'
    # requests:
    #   cpu: 1
    #   memory: 1Gi
    # limits:
    #   cpu: 2
    #   memory: 2Gi

  ## Usage Service Liveness probe configuration
  livenessProbe:
    ## @param usage.livenessProbe.httpGet.path Path to hit for the liveness probe
    ## @param usage.livenessProbe.httpGet.port Port to hit for the liveness probe
    ## @param usage.livenessProbe.httpGet.scheme Scheme to use for the liveness probe
    ## @param usage.livenessProbe.initialDelaySeconds Number of seconds after the container has started before liveness probes are initiated
    ## @param usage.livenessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param usage.livenessProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param usage.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param usage.livenessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed
    failureThreshold: 30
    httpGet:
      path: /health_check
      port: 8005
      scheme: HTTP
    initialDelaySeconds: 15
    periodSeconds: 15
    successThreshold: 1
    timeoutSeconds: 5

  ## Usage Service Readiness probe configuration
  readinessProbe:
    ## @param usage.readinessProbe.httpGet.path Path to hit for the liveness probe
    ## @param usage.readinessProbe.httpGet.port Port to hit for the liveness probe
    ## @param usage.readinessProbe.httpGet.scheme Scheme to use for the liveness probe
    ## @param usage.readinessProbe.initialDelaySeconds Number of seconds after the container has started before liveness probes are initiated
    ## @param usage.readinessProbe.periodSeconds How often (in seconds) to perform the probe
    ## @param usage.readinessProbe.timeoutSeconds Number of seconds after which the probe times out
    ## @param usage.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
    ## @param usage.readinessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed
    failureThreshold: 30
    httpGet:
      path: /health_check
      port: 8005
      scheme: HTTP
    initialDelaySeconds: 15
    periodSeconds: 15
    successThreshold: 1
    timeoutSeconds: 5

  ## Usage HorizontalPodAutoScaler configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  ## The HPA scales by the target CPU utilization percentage by default
  ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
  autoscaling:
    ## @param usage.autoscaling.enabled Enable autoscaling for the Usage Deployment
    ## @param usage.autoscaling.minReplicas Minimum number of replicas for the Usage Deployment
    ## @param usage.autoscaling.maxReplicas Maximum number of replicas for the Usage Deployment
    ## @param usage.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the Usage Deployment
    enabled: false
    minReplicas: 1
    maxReplicas: 4
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  ## @param usage.volumes List of volumes that can be mounted by containers belonging to the pod
  ## @param usage.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
  volumes: []
  volumeMounts: []

  ## @param usage.nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}

  ## @param usage.tolerations Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## @param usage.affinity Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}


temporalParse:
  llamaParse:
    ## @param temporalParse.llamaParse.name Name suffix of the Temporal Parse LlamaParse related resources
    name: temporal-llamaparse

    config:
      ## @param temporalParse.llamaParse.config.debugMode Enable debug mode for Temporal Parse LlamaParse
      debugMode: false
      ## @param temporalParse.llamaParse.config.maxQueueConcurrency Max number of jobs the worker can process at the same time
      maxQueueConcurrency: 3
      ## @param temporalParse.llamaParse.config.maxOldSpaceSize Maximum Node.js old space size in MB
      maxOldSpaceSize: 14500

      ## @param temporalParse.llamaParse.config.openAiApiKey OpenAI API key
      ## @param temporalParse.llamaParse.config.existingOpenAiApiKeySecretName Name of the existing secret to use for the OpenAI API key
      openAiApiKey: ""
      existingOpenAiApiKeySecretName: ""

      ## @param temporalParse.llamaParse.config.azureOpenAi.enabled Enable Azure OpenAI for Temporal Parse LlamaParse
      ## @param temporalParse.llamaParse.config.azureOpenAi.existingSecret Name of the existing secret to use for the Azure OpenAI API key
      ## @param temporalParse.llamaParse.config.azureOpenAi.key Azure OpenAI API key
      ## @param temporalParse.llamaParse.config.azureOpenAi.endpoint Azure OpenAI endpoint
      ## @param temporalParse.llamaParse.config.azureOpenAi.deploymentName Azure OpenAI deployment
      ## @param temporalParse.llamaParse.config.azureOpenAi.apiVersion Azure OpenAI API version
      azureOpenAi:
        enabled: false
        # Note each model has its own deployment name / api version / key on azure openai
        # For more information, please refer to https://docs.cloud.llamaindex.ai/self_hosting/configuration/azure-openai
        existingSecret: ""
        key: ""
        endpoint: ""
        deploymentName: ""
        apiVersion: ""

      ## @param temporalParse.llamaParse.config.anthropicApiKey Anthropic API key
      ## @param temporalParse.llamaParse.config.existingAnthropicApiKeySecret Name of the existing secret to use for the Anthropic API key
      anthropicApiKey: ""
      existingAnthropicApiKeySecret: ""

      ## @param temporalParse.llamaParse.config.geminiApiKey Google Gemini API key
      ## @param temporalParse.llamaParse.config.existingGeminiApiKeySecret Name of the existing secret to use for the Google Gemini API key
      geminiApiKey: ""
      existingGeminiApiKeySecret: ""

      ## @param temporalParse.llamaParse.config.awsBedrock.enabled Enable AWS Bedrock for Temporal Parse LlamaParse
      ## @param temporalParse.llamaParse.config.awsBedrock.existingSecret Name of the existing secret to use for the AWS Bedrock API key
      ## @param temporalParse.llamaParse.config.awsBedrock.region AWS Bedrock region
      ## @param temporalParse.llamaParse.config.awsBedrock.accessKeyId AWS Bedrock access key ID
      ## @param temporalParse.llamaParse.config.awsBedrock.secretAccessKey AWS Bedrock secret access key
      ## @param temporalParse.llamaParse.config.awsBedrock.sonnet3_5ModelVersionName Sonnet 3.5 model version name
      ## @param temporalParse.llamaParse.config.awsBedrock.sonnet3_7ModelVersionName Sonnet 3.7 model version name
      ## @param temporalParse.llamaParse.config.awsBedrock.sonnet4_0ModelVersionName Sonnet 4.0 model version name
      ## @param temporalParse.llamaParse.config.awsBedrock.haiku3_5_ModelVersionName Haiku 3.5 model version name
      awsBedrock:
        enabled: false
        existingSecret: ""
        region: ""
        accessKeyId: ""
        secretAccessKey: ""
        sonnet3_5ModelVersionName: "anthropic.claude-3-5-sonnet-20240620-v1:0"
        sonnet3_7ModelVersionName: "anthropic.claude-3-7-sonnet-20250219-v1:0"
        sonnet4_0ModelVersionName: "anthropic.claude-sonnet-4-20250514-v1:0"
        haiku3_5_ModelVersionName: "anthropic.claude-3-5-haiku-20241022-v1:0"

      ## @param temporalParse.llamaParse.config.googleVertexAi.enabled Enable Google Vertex AI for Temporal Parse LlamaParse
      ## @param temporalParse.llamaParse.config.googleVertexAi.existingSecret Name of the existing secret to use for the Google Vertex AI API key
      ## @param temporalParse.llamaParse.config.googleVertexAi.projectId Google Vertex AI project id
      ## @param temporalParse.llamaParse.config.googleVertexAi.location Google Vertex AI location
      ## @param temporalParse.llamaParse.config.googleVertexAi.credentialsJson Google Vertex AI credentials JSON
      googleVertexAi:
        enabled: false
        existingSecret: ""
        projectId: ""
        location: ""
        credentialsJson: ""

      ## @param temporalParse.llamaParse.config.s3UploadBucket S3 bucket to upload files to
      ## @param temporalParse.llamaParse.config.s3OutputBucket S3 bucket to output files to
      s3UploadBucket: "llama-platform-file-parsing"
      s3OutputBucket: "llama-platform-file-parsing"

    ## @param temporalParse.llamaParse.replicas Number of replicas of Temporal Parse LlamaParse Deployment
    replicas: 2

    ## Temporal Parse LlamaParse Image information
    ## @param temporalParse.llamaParse.image.registry Temporal Parse LlamaParse Image registry
    ## @param temporalParse.llamaParse.image.repository Temporal Parse LlamaParse Image repository
    ## @param temporalParse.llamaParse.image.tag Temporal Parse LlamaParse Image tag
    ## @param temporalParse.llamaParse.image.pullPolicy Temporal Parse LlamaParse Image pull policy
    image:
      registry: docker.io
      repository: llamaindex/llamacloud-llamaparse
      tag: 0.5.7
      pullPolicy: IfNotPresent

    ## ServiceAccount configuration
    ## @param temporalParse.llamaParse.serviceAccount.create Whether or not to create a new service account
    ## @param temporalParse.llamaParse.serviceAccount.name Name of the service account
    ## @param temporalParse.llamaParse.serviceAccount.labels Labels to add to the service account
    ## @param temporalParse.llamaParse.serviceAccount.annotations Annotations to add to the service account
    ## @param temporalParse.llamaParse.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
    serviceAccount:
      create: true
      name: ""
      labels: {}
      annotations: {}
      automountServiceAccountToken: true

    ## @param temporalParse.llamaParse.labels Labels added to the Temporal Parse LlamaParse Deployment.
    labels: {}
    ## @param temporalParse.llamaParse.annotations Annotations added to the Temporal Parse LlamaParse Deployment.
    annotations: {}

    ## @param temporalParse.llamaParse.containerPort Port to expose on the Temporal Parse LlamaParse container
    ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
    containerPort: 8004

    ## @param temporalParse.llamaParse.service.type Temporal Parse LlamaParse Service type
    ## @param temporalParse.llamaParse.service.port Temporal Parse LlamaParse Service port
    service:
      type: ClusterIP
      port: 8004

    ## @param temporalParse.llamaParse.extraEnvVariables Extra environment variables to add to Temporal Parse llamaParse pods
    ## Example:
    ## extraEnvVariables:
    ##   - name: FOO
    ##     value: BAR
    ##   - name: OPENAI_API_KEY
    ##     valueFrom:
    ##       secretKeyRef:
    ##         name: openai-api-secret
    ##         key: openai-api-key
    extraEnvVariables: []

    ## External Secrets Configuration
    ## @param temporalParse.llamaParse.externalSecrets.enabled Enable external secrets for the Temporal Parse LlamaParse Deployment
    ## @param temporalParse.llamaParse.externalSecrets.secrets List of external secrets to load environment variables from
    externalSecrets:
      ## @param temporalParse.llamaParse.externalSecrets.enabled Enable external secrets for the Temporal Parse LlamaParse Deployment
      enabled: false
      ## @param temporalParse.llamaParse.externalSecrets.secrets List of external secrets to load environment variables from
      ## ["secret-1", "secret-2"]
      secrets: []

    ## @param temporalParse.llamaParse.podAnnotations Annotations to add to the resulting Pods of the Deployment.
    podAnnotations: {}

    ## @param temporalParse.llamaParse.podLabels Labels to add to the resulting Pods of the Deployment.
    podLabels: {}

    ## @param temporalParse.llamaParse.podSecurityContext Pod security context
    podSecurityContext: {}

    ## @param temporalParse.llamaParse.securityContext Security context for the container
    securityContext: {}
    #  runAsUser: 1000
    #  runAsGroup: 1000

    ## Temporal Parse LlamaParse resources: Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    ## Below are the recommended values for Temporal Parse LlamaParse. Please adjust according to your deployment's needs.
    resources:
      ## @param temporalParse.llamaParse.resources.requests.memory Memory request for the Temporal Parse LlamaParse container
      ## @param temporalParse.llamaParse.resources.requests.cpu CPU request for the Temporal Parse LlamaParse container
      ## @param temporalParse.llamaParse.resources.limits.memory Memory limit for the Temporal Parse LlamaParse container
      ## @param temporalParse.llamaParse.resources.limits.cpu CPU limit for the Temporal Parse LlamaParse container
      requests:
        memory: 6Gi
        cpu: 3
      limits:
        memory: 13Gi
        cpu: 7

    ## Temporal Parse LlamaParse Liveness probe configuration
    livenessProbe:
      ## @param temporalParse.llamaParse.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
      ## @param temporalParse.llamaParse.livenessProbe.httpGet.path Path to hit for the liveness probe
      ## @param temporalParse.llamaParse.livenessProbe.httpGet.port Port to hit for the liveness probe
      ## @param temporalParse.llamaParse.livenessProbe.initialDelaySeconds Number of seconds after the container has started before liveness probes are initiated
      ## @param temporalParse.llamaParse.livenessProbe.periodSeconds How often (in seconds) to perform the probe
      ## @param temporalParse.llamaParse.livenessProbe.timeoutSeconds Number of seconds after which the probe times out
      failureThreshold: 10
      httpGet:
        path: /healthcheck
        port: 8004
      initialDelaySeconds: 30
      periodSeconds: 30
      timeoutSeconds: 5

    ## Temporal Parse LlamaParse HorizontalPodAutoScaler configuration
    ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
    ## The HPA scales by the target CPU utilization percentage by default
    ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
    autoscaling:
      ## @param temporalParse.llamaParse.autoscaling.enabled Enable autoscaling for the Temporal Parse LlamaParse Deployment
      ## @param temporalParse.llamaParse.autoscaling.minReplicas Minimum number of replicas for the Temporal Parse LlamaParse Deployment
      ## @param temporalParse.llamaParse.autoscaling.maxReplicas Maximum number of replicas for the Temporal Parse LlamaParse Deployment
      ## @param temporalParse.llamaParse.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the Temporal Parse LlamaParse Deployment
      enabled: true
      minReplicas: 2
      maxReplicas: 10
      targetCPUUtilizationPercentage: 80
      # targetMemoryUtilizationPercentage: 80

    ## KEDA configuration for the Temporal Parse LlamaParse Deployment
    ## Ref: https://keda.sh
    ## Note: .Values.temporalParse.llamaParse.autoscaling.enabled must be false for KEDA to be enabled
    keda:
      ## @param temporalParse.llamaParse.keda.enabled Enable KEDA for the Temporal Parse llamaParse Deployment
      enabled: false
      ## @param temporalParse.llamaParse.keda.additionalAnnotations Additional annotations for the KEDA
      additionalAnnotations: {}
      ## @param temporalParse.llamaParse.keda.additionalLabels Additional labels for the KEDA
      additionalLabels: {}
      ## @param temporalParse.llamaParse.keda.pollingInterval Polling interval for the KEDA
      pollingInterval: 15
      ## @param temporalParse.llamaParse.keda.cooldownPeriod Cooldown period for the KEDA
      cooldownPeriod: 120
      ## @param temporalParse.llamaParse.keda.minReplicaCount Minimum number of replicas for the KEDA
      minReplicaCount: 2
      ## @param temporalParse.llamaParse.keda.maxReplicaCount Maximum number of replicas for the KEDA
      maxReplicaCount: 10
      ## @param temporalParse.llamaParse.keda.initialCooldownPeriod Initial cooldown period for the KEDA
      initialCooldownPeriod: 0
      ## @param temporalParse.llamaParse.keda.fallback Fallback for the KEDA
      fallback: {}
      ## @param temporalParse.llamaParse.keda.advanced Advanced configuration for the KEDA
      advanced: {}
      ## temporalParse.llamaParse.keda.triggers: Triggers for the KEDA
      ## This cannot be empty if .Values.temporalParse.llamaParse.keda.enabled is true
      ## Default trigger uses Temporal scaler: https://keda.sh/docs/2.17/scalers/temporal/
      ## NOTE: You must update the taskQueue value for your specific worker queue name
      ## @param temporalParse.llamaParse.keda.triggers[0].type Trigger type (temporal)
      ## @param temporalParse.llamaParse.keda.triggers[0].metadata.taskQueue Temporal task queue name for this worker
      ## @param temporalParse.llamaParse.keda.triggers[0].metadata.targetQueueSize Target queue size for scaling
      ## @param temporalParse.llamaParse.keda.triggers[0].metadata.activationTargetQueueSize Activation target queue size for scaling
      ## @param temporalParse.llamaParse.keda.triggers[0].metadata.endpoint Temporal endpoint URL
      triggers:
        - type: temporal
          metadata:
            taskQueue: "parse-activity-queue"
            ## Target queue size (from k8sctl defaults: BaseScaledObjectConfig.target_queue_size)
            targetQueueSize: "2"
            ## Activation target queue size (from k8sctl defaults: BaseScaledObjectConfig.activation_target_queue_size)
            activationTargetQueueSize: "0"
            ## Temporal namespace (optional) - the Temporal workflow namespace, defaults to "default" if not set
            ## Uncomment to specify a custom Temporal namespace:
            # namespace: "default"
            ## Temporal endpoint - dynamically resolves to internal or external temporal service
            endpoint: "{{ printf \"%s:%s\" (include \"temporal.host\" .) (include \"temporal.port\" .) }}"

    ## PodDisruptionBudget configuration
    ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
    podDisruptionBudget:
      ## @param temporalParse.llamaParse.podDisruptionBudget.enabled Enable PodDisruptionBudget for the Temporal Parse LlamaParse Deployment
      ## @param temporalParse.llamaParse.podDisruptionBudget.maxUnavailable Maximum number of unavailable pods
      enabled: true
      maxUnavailable: 1

    ## @param temporalParse.llamaParse.volumes List of volumes that can be mounted by containers belonging to the pod
    ## @param temporalParse.llamaParse.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
    volumes: []
    volumeMounts: []

    ## @param temporalParse.llamaParse.nodeSelector Node labels for pod assignment
    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    nodeSelector: {}

    ## @param temporalParse.llamaParse.tolerations Taints to tolerate on node assignment:
    ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []

    ## @param temporalParse.llamaParse.affinity Pod scheduling constraints
    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
    affinity: {}

    metrics:
      ## @param temporalParse.llamaParse.metrics.enabled Enable metrics for the Temporal Parse llamaParse
      enabled: false
      serviceMonitor:
        ## @param temporalParse.llamaParse.metrics.serviceMonitor.enabled Enable service monitor for the Temporal Parse llamaParse
        enabled: false
        ## @param temporalParse.llamaParse.metrics.serviceMonitor.selector Selector for the service monitor
        selector: {}
        ## @param temporalParse.llamaParse.metrics.serviceMonitor.additionalLabels Additional labels for the service monitor
        additionalLabels: {}
        ## @param temporalParse.llamaParse.metrics.serviceMonitor.annotations Annotations for the service monitor
        annotations: {}
        ## @param temporalParse.llamaParse.metrics.serviceMonitor.interval Interval for the service monitor
        interval: 30s
        ## @param temporalParse.llamaParse.metrics.serviceMonitor.scrapeTimeout Timeout for the service monitor
        scrapeTimeout: 15s
        ## @param temporalParse.llamaParse.metrics.serviceMonitor.relabelings Relabelings for the service monitor
        relabelings: []
        ## @param temporalParse.llamaParse.metrics.serviceMonitor.metricRelabelings Metric relabelings for the service monitor
        metricRelabelings: []
        ## @param temporalParse.llamaParse.metrics.serviceMonitor.scheme Scheme for the service monitor
        scheme: http
        ## @param temporalParse.llamaParse.metrics.serviceMonitor.tlsConfig TLS configuration for the service monitor
        tlsConfig: {}

      rules:
        ## @param temporalParse.llamaParse.metrics.rules.enabled Enable rules for the Temporal Parse llamaParse
        enabled: false
        ## @param temporalParse.llamaParse.metrics.rules.namespace Namespace for the rules
        namespace: ""
        ## @param temporalParse.llamaParse.metrics.rules.additionalLabels Additional labels for the rules
        additionalLabels: {}
        ## @param temporalParse.llamaParse.metrics.rules.annotations Annotations for the rules
        annotations: {}
        ## @param temporalParse.llamaParse.metrics.rules.spec Rules spec
        spec: []

  parseDelegate:
    ## @param temporalParse.parseDelegate.name Name suffix of the Temporal Parse Delegate related resources
    name: temporal-parse-delegate

    ## @param temporalParse.parseDelegate.command Command to run in the container
    command: ["temporal_parse_delegate"]

    ## @param temporalParse.parseDelegate.replicas Number of replicas of Temporal Parse Delegate Deployment
    replicas: 1

    ## Temporal Parse Delegate configuration
    ## @param temporalParse.parseDelegate.config.logLevel Log level for the Temporal Parse Delegate
    config:
      logLevel: "info"

    ## Temporal Parse Delegate Image information
    ## @param temporalParse.parseDelegate.image.registry Temporal Parse Delegate Image registry
    ## @param temporalParse.parseDelegate.image.repository Temporal Parse Delegate Image repository
    ## @param temporalParse.parseDelegate.image.tag Temporal Parse Delegate Image tag
    ## @param temporalParse.parseDelegate.image.pullPolicy Temporal Parse Delegate Image pull policy
    image:
      registry: docker.io
      repository: llamaindex/llamacloud-backend
      tag: 0.5.7
      pullPolicy: IfNotPresent

    ## ServiceAccount configuration
    ## @param temporalParse.parseDelegate.serviceAccount.create Whether or not to create a new service account
    ## @param temporalParse.parseDelegate.serviceAccount.name Name of the service account
    ## @param temporalParse.parseDelegate.serviceAccount.labels Labels to add to the service account
    ## @param temporalParse.parseDelegate.serviceAccount.annotations Annotations to add to the service account
    ## @param temporalParse.parseDelegate.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
    serviceAccount:
      create: true
      name: ""
      labels: {}
      annotations: {}
      automountServiceAccountToken: true

    ## @param temporalParse.parseDelegate.containerPort Port to expose on the Temporal Parse Delegate container
    ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
    containerPort: 8000

    ## Temporal Parse Delegate Service information
    ## @param temporalParse.parseDelegate.service.type Temporal Parse Delegate Service type
    ## @param temporalParse.parseDelegate.service.port Temporal Parse Delegate Service port
    ## @param temporalParse.parseDelegate.service.labels Labels to add to the service
    ## @param temporalParse.parseDelegate.service.annotations Annotations to add to the service
    service:
      type: ClusterIP
      port: 8000
      labels: {}
      annotations: {}

    ## @param temporalParse.parseDelegate.labels Labels added to the Temporal Parse Delegate Deployment.
    labels: {}
    ## @param temporalParse.parseDelegate.annotations Annotations added to the Temporal Parse Delegate Deployment.
    annotations: {}

    ## @param temporalParse.parseDelegate.extraEnvVariables Extra environment variables to add to Temporal Parse Delegate pods
    ## Example:
    ## extraEnvVariables:
    ##   - name: FOO
    ##     value: BAR
    extraEnvVariables: []

    ## External Secrets Configuration
    ## @param temporalParse.parseDelegate.externalSecrets.enabled Enable external secrets for the Temporal Parse Delegate Deployment
    ## @param temporalParse.parseDelegate.externalSecrets.secrets List of external secrets to load environment variables from
    externalSecrets:
      enabled: false
      ## ["secret-1", "secret-2"]
      secrets: []

    ## @param temporalParse.parseDelegate.podAnnotations Annotations to add to the resulting Pods of the Deployment.
    podAnnotations: {}

    ## @param temporalParse.parseDelegate.podLabels Labels to add to the resulting Pods of the Deployment.
    podLabels: {}

    ## @param temporalParse.parseDelegate.podSecurityContext Pod security context
    podSecurityContext: {}

    ## @param temporalParse.parseDelegate.securityContext Security context for the container
    securityContext: {}
    #  runAsUser: 1000
    #  runAsGroup: 1000

    ## @param temporalParse.parseDelegate.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    resources: {}
      ## To use the recommended values, uncomment the following lines and remove the curly braces after 'resources:'
      # requests:
      #   cpu: 1
      #   memory: 1Gi
      # limits:
      #   cpu: 2
      #   memory: 2Gi

    ## Temporal Parse Delegate Liveness probe configuration
    livenessProbe:
      ## @param temporalParse.parseDelegate.livenessProbe.httpGet.path Path to hit for the liveness probe
      ## @param temporalParse.parseDelegate.livenessProbe.httpGet.port Port to hit for the liveness probe
      ## @param temporalParse.parseDelegate.livenessProbe.periodSeconds How often (in seconds) to perform the probe
      ## @param temporalParse.parseDelegate.livenessProbe.timeoutSeconds Number of seconds after which the probe times out
      ## @param temporalParse.parseDelegate.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
      httpGet:
        path: /healthcheck
        port: 8000
      periodSeconds: 30
      timeoutSeconds: 10
      failureThreshold: 5

    ## Temporal Parse Delegate HorizontalPodAutoScaler configuration
    ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
    ## The HPA scales by the target CPU utilization percentage by default
    ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
    autoscaling:
      ## @param temporalParse.parseDelegate.autoscaling.enabled Enable autoscaling for the Temporal Parse Delegate Deployment
      ## @param temporalParse.parseDelegate.autoscaling.minReplicas Minimum number of replicas for the Temporal Parse Delegate Deployment
      ## @param temporalParse.parseDelegate.autoscaling.maxReplicas Maximum number of replicas for the Temporal Parse Delegate Deployment
      ## @param temporalParse.parseDelegate.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the Temporal Parse Delegate Deployment
      enabled: false
      minReplicas: 1
      maxReplicas: 4
      targetCPUUtilizationPercentage: 80
      # targetMemoryUtilizationPercentage: 80

    ## KEDA based autoscaling for Temporal Parse Delegate Deployment
    ## Ref: https://keda.sh
    ## Note: .Values.temporalParse.parseDelegate.autoscaling.enabled must be false for KEDA to be enabled
    keda:
      ## @param temporalParse.parseDelegate.keda.enabled Enable KEDA for the Temporal Parse Delegate Deployment
      enabled: false
      ## @param temporalParse.parseDelegate.keda.additionalAnnotations Additional annotations for the KEDA
      additionalAnnotations: {}
      ## @param temporalParse.parseDelegate.keda.additionalLabels Additional labels for the KEDA
      additionalLabels: {}
      ## @param temporalParse.parseDelegate.keda.pollingInterval Polling interval for the KEDA
      pollingInterval: 15
      ## @param temporalParse.parseDelegate.keda.cooldownPeriod Cooldown period for the KEDA
      cooldownPeriod: 120
      ## @param temporalParse.parseDelegate.keda.minReplicaCount Minimum number of replicas for the KEDA
      minReplicaCount: 1
      ## @param temporalParse.parseDelegate.keda.maxReplicaCount Maximum number of replicas for the KEDA
      maxReplicaCount: 10
      ## @param temporalParse.parseDelegate.keda.initialCooldownPeriod Initial cooldown period for the KEDA
      initialCooldownPeriod: 0
      ## @param temporalParse.parseDelegate.keda.fallback Fallback configuration for the KEDA
      ## Ref: https://keda.sh/docs/2.10/concepts/scaling-deployments/#fallback
      fallback: {}
        # failureThreshold: 3
        # replicas: 6
      ## @param temporalParse.parseDelegate.keda.advanced Advanced configuration for the KEDA
      ## Ref: https://keda.sh/docs/2.10/concepts/scaling-deployments/#advanced
      advanced: {}
      ## temporalParse.parseDelegate.keda.triggers: Triggers for the KEDA
      ## Default trigger uses Temporal scaler: https://keda.sh/docs/2.17/scalers/temporal/
      ## NOTE: You must update the taskQueue value for your specific worker queue name
      ## @param temporalParse.parseDelegate.keda.triggers[0].type Trigger type (temporal)
      ## @param temporalParse.parseDelegate.keda.triggers[0].metadata.taskQueue Temporal task queue name for this worker
      ## @param temporalParse.parseDelegate.keda.triggers[0].metadata.targetQueueSize Target queue size for scaling
      ## @param temporalParse.parseDelegate.keda.triggers[0].metadata.activationTargetQueueSize Activation target queue size for scaling
      ## @param temporalParse.parseDelegate.keda.triggers[0].metadata.endpoint Temporal endpoint URL
      triggers:
        - type: temporal
          metadata:
            taskQueue: "parse"
            ## Target queue size (from k8sctl defaults: BaseScaledObjectConfig.target_queue_size)
            targetQueueSize: "2"
            ## Activation target queue size (from k8sctl defaults: BaseScaledObjectConfig.activation_target_queue_size)
            activationTargetQueueSize: "0"
            ## Temporal namespace (optional) - the Temporal workflow namespace, defaults to "default" if not set
            ## Uncomment to specify a custom Temporal namespace:
            # namespace: "default"
            ## Temporal endpoint - dynamically resolves to internal or external temporal service
            endpoint: "{{ printf \"%s:%s\" (include \"temporal.host\" .) (include \"temporal.port\" .) }}"

    ## Prometheus Metrics Configuration
    metrics:
      ## @param temporalParse.parseDelegate.metrics.enabled Enable exposing metrics to be gathered by Prometheus
      enabled: false
      serviceMonitor:
        ## @param temporalParse.parseDelegate.metrics.serviceMonitor.enabled Create ServiceMonitor resource for scraping metrics using PrometheusOperator
        enabled: false
        ## @param temporalParse.parseDelegate.metrics.serviceMonitor.selector ServiceMonitor selector for the labels to be scraped
        selector: {}
        ## @param temporalParse.parseDelegate.metrics.serviceMonitor.additionalLabels Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with
        additionalLabels: {}
        ## @param temporalParse.parseDelegate.metrics.serviceMonitor.annotations ServiceMonitor annotations
        annotations: {}
        ## @param temporalParse.parseDelegate.metrics.serviceMonitor.interval Interval at which metrics should be scraped
        interval: 30s
        ## @param temporalParse.parseDelegate.metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended
        scrapeTimeout: 15s
        ## @param temporalParse.parseDelegate.metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
        relabelings: []
        ## @param temporalParse.parseDelegate.metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
        metricRelabelings: []
        ## @param temporalParse.parseDelegate.metrics.serviceMonitor.scheme HTTP scheme to use for scraping
        scheme: http
        ## @param temporalParse.parseDelegate.metrics.serviceMonitor.tlsConfig TLS configuration to use when scraping the endpoint
        tlsConfig: {}

    ## @param temporalParse.parseDelegate.volumes List of volumes that can be mounted by containers belonging to the pod
    ## @param temporalParse.parseDelegate.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
    volumes: []
    volumeMounts: []

    ## @param temporalParse.parseDelegate.nodeSelector Node labels for pod assignment
    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    nodeSelector: {}

    ## @param temporalParse.parseDelegate.tolerations Taints to tolerate on node assignment:
    ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []

    ## @param temporalParse.parseDelegate.affinity Pod scheduling constraints
    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
    affinity: {}

  parseScreenshotPDFs:
    ## @param temporalParse.parseScreenshotPDFs.name Name suffix of the Temporal Parse Screenshot PDFs related resources
    name: temporal-parse-screenshot-pdfs

    ## @param temporalParse.parseScreenshotPDFs.command Command to run in the container
    command: ["temporal_parse_screenshots_pdf"]

    ## @param temporalParse.parseScreenshotPDFs.replicas Number of replicas of Temporal Parse Screenshot PDFs Deployment
    replicas: 1

    ## Temporal Parse Screenshot PDFs configuration
    ## @param temporalParse.parseScreenshotPDFs.config.logLevel Log level for the Temporal Parse Screenshot PDFs
    config:
      logLevel: "info"

    ## Temporal Parse Screenshot PDFs Image information
    ## @param temporalParse.parseScreenshotPDFs.image.registry Temporal Parse Screenshot PDFs Image registry
    ## @param temporalParse.parseScreenshotPDFs.image.repository Temporal Parse Screenshot PDFs Image repository
    ## @param temporalParse.parseScreenshotPDFs.image.tag Temporal Parse Screenshot PDFs Image tag
    ## @param temporalParse.parseScreenshotPDFs.image.pullPolicy Temporal Parse Screenshot PDFs Image pull policy
    image:
      registry: docker.io
      repository: llamaindex/llamacloud-backend
      tag: 0.5.7
      pullPolicy: IfNotPresent

    ## ServiceAccount configuration
    ## @param temporalParse.parseScreenshotPDFs.serviceAccount.create Whether or not to create a new service account
    ## @param temporalParse.parseScreenshotPDFs.serviceAccount.name Name of the service account
    ## @param temporalParse.parseScreenshotPDFs.serviceAccount.labels Labels to add to the service account
    ## @param temporalParse.parseScreenshotPDFs.serviceAccount.annotations Annotations to add to the service account
    ## @param temporalParse.parseScreenshotPDFs.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
    serviceAccount:
      create: true
      name: ""
      labels: {}
      annotations: {}
      automountServiceAccountToken: true

    ## @param temporalParse.parseScreenshotPDFs.containerPort Port to expose on the Temporal Parse Screenshot PDFs container
    ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
    containerPort: 8000

    ## Temporal Parse Screenshot PDFs Service information
    ## @param temporalParse.parseScreenshotPDFs.service.type Temporal Parse Screenshot PDFs Service type
    ## @param temporalParse.parseScreenshotPDFs.service.port Temporal Parse Screenshot PDFs Service port
    ## @param temporalParse.parseScreenshotPDFs.service.labels Labels to add to the service
    ## @param temporalParse.parseScreenshotPDFs.service.annotations Annotations to add to the service
    service:
      type: ClusterIP
      port: 8000
      labels: {}
      annotations: {}

    ## @param temporalParse.parseScreenshotPDFs.labels Labels added to the Temporal Parse Screenshot PDFs Deployment.
    labels: {}
    ## @param temporalParse.parseScreenshotPDFs.annotations Annotations added to the Temporal Parse Screenshot PDFs Deployment.
    annotations: {}

    ## @param temporalParse.parseScreenshotPDFs.extraEnvVariables Extra environment variables to add to Temporal Parse Screenshot PDFs pods
    ## Example:
    ## extraEnvVariables:
    ##   - name: FOO
    ##     value: BAR
    extraEnvVariables: []

    ## External Secrets Configuration
    ## @param temporalParse.parseScreenshotPDFs.externalSecrets.enabled Enable external secrets for the Temporal Parse Screenshot PDFs Deployment
    ## @param temporalParse.parseScreenshotPDFs.externalSecrets.secrets List of external secrets to load environment variables from
    externalSecrets:
      enabled: false
      ## ["secret-1", "secret-2"]
      secrets: []

    ## @param temporalParse.parseScreenshotPDFs.podAnnotations Annotations to add to the resulting Pods of the Deployment.
    podAnnotations: {}

    ## @param temporalParse.parseScreenshotPDFs.podLabels Labels to add to the resulting Pods of the Deployment.
    podLabels: {}

    ## @param temporalParse.parseScreenshotPDFs.podSecurityContext Pod security context
    podSecurityContext: {}

    ## @param temporalParse.parseScreenshotPDFs.securityContext Security context for the container
    securityContext: {}
    #  runAsUser: 1000
    #  runAsGroup: 1000

    ## @param temporalParse.parseScreenshotPDFs.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    resources: {}
      ## To use the recommended values, uncomment the following lines and remove the curly braces after 'resources:'
      # requests:
      #   cpu: 1
      #   memory: 1Gi
      # limits:
      #   cpu: 2
      #   memory: 2Gi

    ## Temporal Parse Screenshot PDFs Liveness probe configuration
    livenessProbe:
      ## @param temporalParse.parseScreenshotPDFs.livenessProbe.httpGet.path Path to hit for the liveness probe
      ## @param temporalParse.parseScreenshotPDFs.livenessProbe.httpGet.port Port to hit for the liveness probe
      ## @param temporalParse.parseScreenshotPDFs.livenessProbe.periodSeconds How often (in seconds) to perform the probe
      ## @param temporalParse.parseScreenshotPDFs.livenessProbe.timeoutSeconds Number of seconds after which the probe times out
      ## @param temporalParse.parseScreenshotPDFs.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
      httpGet:
        path: /healthcheck
        port: 8000
      periodSeconds: 30
      timeoutSeconds: 10
      failureThreshold: 5

    ## Temporal Parse Screenshot PDFs HorizontalPodAutoScaler configuration
    ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
    ## The HPA scales by the target CPU utilization percentage by default
    ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
    autoscaling:
      ## @param temporalParse.parseScreenshotPDFs.autoscaling.enabled Enable autoscaling for the Temporal Parse Screenshot PDFs Deployment
      ## @param temporalParse.parseScreenshotPDFs.autoscaling.minReplicas Minimum number of replicas for the Temporal Parse Screenshot PDFs Deployment
      ## @param temporalParse.parseScreenshotPDFs.autoscaling.maxReplicas Maximum number of replicas for the Temporal Parse Screenshot PDFs Deployment
      ## @param temporalParse.parseScreenshotPDFs.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the Temporal Parse Screenshot PDFs Deployment
      enabled: false
      minReplicas: 1
      maxReplicas: 4
      targetCPUUtilizationPercentage: 80
      # targetMemoryUtilizationPercentage: 80

    ## KEDA based autoscaling for Temporal Parse Screenshot PDFs Deployment
    ## Ref: https://keda.sh
    ## Note: .Values.temporalParse.parseScreenshotPDFs.autoscaling.enabled must be false for KEDA to be enabled
    keda:
      ## @param temporalParse.parseScreenshotPDFs.keda.enabled Enable KEDA for the Temporal Parse Screenshot PDFs Deployment
      enabled: false
      ## @param temporalParse.parseScreenshotPDFs.keda.additionalAnnotations Additional annotations for the KEDA
      additionalAnnotations: {}
      ## @param temporalParse.parseScreenshotPDFs.keda.additionalLabels Additional labels for the KEDA
      additionalLabels: {}
      ## @param temporalParse.parseScreenshotPDFs.keda.pollingInterval Polling interval for the KEDA
      pollingInterval: 15
      ## @param temporalParse.parseScreenshotPDFs.keda.cooldownPeriod Cooldown period for the KEDA
      cooldownPeriod: 120
      ## @param temporalParse.parseScreenshotPDFs.keda.minReplicaCount Minimum number of replicas for the KEDA
      minReplicaCount: 1
      ## @param temporalParse.parseScreenshotPDFs.keda.maxReplicaCount Maximum number of replicas for the KEDA
      maxReplicaCount: 10
      ## @param temporalParse.parseScreenshotPDFs.keda.initialCooldownPeriod Initial cooldown period for the KEDA
      initialCooldownPeriod: 0
      ## @param temporalParse.parseScreenshotPDFs.keda.fallback Fallback configuration for the KEDA
      ## Ref: https://keda.sh/docs/2.10/concepts/scaling-deployments/#fallback
      fallback: {}
        # failureThreshold: 3
        # replicas: 6
      ## @param temporalParse.parseScreenshotPDFs.keda.advanced Advanced configuration for the KEDA
      ## Ref: https://keda.sh/docs/2.10/concepts/scaling-deployments/#advanced
      advanced: {}
      ## temporalParse.parseScreenshotPDFs.keda.triggers: Triggers for the KEDA
      ## Default trigger uses Temporal scaler: https://keda.sh/docs/2.17/scalers/temporal/
      ## NOTE: You must update the taskQueue value for your specific worker queue name
      ## @param temporalParse.parseScreenshotPDFs.keda.triggers[0].type Trigger type (temporal)
      ## @param temporalParse.parseScreenshotPDFs.keda.triggers[0].metadata.taskQueue Temporal task queue name for this worker
      ## @param temporalParse.parseScreenshotPDFs.keda.triggers[0].metadata.targetQueueSize Target queue size for scaling
      ## @param temporalParse.parseScreenshotPDFs.keda.triggers[0].metadata.activationTargetQueueSize Activation target queue size for scaling
      ## @param temporalParse.parseScreenshotPDFs.keda.triggers[0].metadata.endpoint Temporal endpoint URL
      triggers:
        - type: temporal
          metadata:
            taskQueue: "parse-screenshot-pdf"
            ## Target queue size (from k8sctl defaults: BaseScaledObjectConfig.target_queue_size)
            targetQueueSize: "2"
            ## Activation target queue size (from k8sctl defaults: BaseScaledObjectConfig.activation_target_queue_size)
            activationTargetQueueSize: "0"
            ## Temporal namespace (optional) - the Temporal workflow namespace, defaults to "default" if not set
            ## Uncomment to specify a custom Temporal namespace:
            # namespace: "default"
            ## Temporal endpoint - dynamically resolves to internal or external temporal service
            endpoint: "{{ printf \"%s:%s\" (include \"temporal.host\" .) (include \"temporal.port\" .) }}"

    ## Prometheus Metrics Configuration
    metrics:
      ## @param temporalParse.parseScreenshotPDFs.metrics.enabled Enable exposing metrics to be gathered by Prometheus
      enabled: false
      serviceMonitor:
        ## @param temporalParse.parseScreenshotPDFs.metrics.serviceMonitor.enabled Create ServiceMonitor resource for scraping metrics using PrometheusOperator
        enabled: false
        ## @param temporalParse.parseScreenshotPDFs.metrics.serviceMonitor.selector ServiceMonitor selector for the labels to be scraped
        selector: {}
        ## @param temporalParse.parseScreenshotPDFs.metrics.serviceMonitor.additionalLabels Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with
        additionalLabels: {}
        ## @param temporalParse.parseScreenshotPDFs.metrics.serviceMonitor.annotations ServiceMonitor annotations
        annotations: {}
        ## @param temporalParse.parseScreenshotPDFs.metrics.serviceMonitor.interval Interval at which metrics should be scraped
        interval: 30s
        ## @param temporalParse.parseScreenshotPDFs.metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended
        scrapeTimeout: 15s
        ## @param temporalParse.parseScreenshotPDFs.metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
        relabelings: []
        ## @param temporalParse.parseScreenshotPDFs.metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
        metricRelabelings: []
        ## @param temporalParse.parseScreenshotPDFs.metrics.serviceMonitor.scheme HTTP scheme to use for scraping
        scheme: http
        ## @param temporalParse.parseScreenshotPDFs.metrics.serviceMonitor.tlsConfig TLS configuration to use when scraping the endpoint
        tlsConfig: {}

    ## @param temporalParse.parseScreenshotPDFs.volumes List of volumes that can be mounted by containers belonging to the pod
    ## @param temporalParse.parseScreenshotPDFs.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
    volumes: []
    volumeMounts: []

    ## @param temporalParse.parseScreenshotPDFs.nodeSelector Node labels for pod assignment
    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    nodeSelector: {}

    ## @param temporalParse.parseScreenshotPDFs.tolerations Taints to tolerate on node assignment:
    ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []

    ## @param temporalParse.parseScreenshotPDFs.affinity Pod scheduling constraints
    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
    affinity: {}

## @section Temporal Jobs Service Configuration
temporalJobsService:
  api:
    ## @param temporalJobsService.api.name Name suffix of the Temporal Jobs Service API related resources
    name: temporal-jobs-api

    ## @param temporalJobsService.api.command Command to run in the container
    command: ["start_jobs_api"]

    ## @param temporalJobsService.api.replicas Number of replicas of Temporal Jobs Service API Deployment
    replicas: 1

    ## Temporal Jobs Service API configuration
    ## @param temporalJobsService.api.config.logLevel Log level for the Temporal Jobs Service API
    config:
      logLevel: "info"

    ## Temporal Jobs Service API Image information
    ## @param temporalJobsService.api.image.registry Temporal Jobs Service API Image registry
    ## @param temporalJobsService.api.image.repository Temporal Jobs Service API Image repository
    ## @param temporalJobsService.api.image.tag Temporal Jobs Service API Image tag
    ## @param temporalJobsService.api.image.pullPolicy Temporal Jobs Service API Image pull policy
    image:
      registry: docker.io
      repository: llamaindex/llamacloud-backend
      tag: 0.5.7
      pullPolicy: IfNotPresent

    ## ServiceAccount configuration
    ## @param temporalJobsService.api.serviceAccount.create Whether or not to create a new service account
    ## @param temporalJobsService.api.serviceAccount.name Name of the service account
    ## @param temporalJobsService.api.serviceAccount.labels Labels to add to the service account
    ## @param temporalJobsService.api.serviceAccount.annotations Annotations to add to the service account
    ## @param temporalJobsService.api.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
    serviceAccount:
      create: true
      name: ""
      labels: {}
      annotations: {}
      automountServiceAccountToken: true

    ## @param temporalJobsService.api.containerPort Port to expose on the Temporal Jobs Service API container
    ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
    containerPort: 8002

    ## Temporal Jobs Service API Service information
    ## @param temporalJobsService.api.service.type Temporal Jobs Service API Service type
    ## @param temporalJobsService.api.service.port Temporal Jobs Service API Service port
    ## @param temporalJobsService.api.service.labels Labels to add to the service
    ## @param temporalJobsService.api.service.annotations Annotations to add to the service
    service:
      type: ClusterIP
      port: 8002
      labels: {}
      annotations: {}

    ## @param temporalJobsService.api.labels Labels added to the Temporal Jobs Service API Deployment.
    labels: {}
    ## @param temporalJobsService.api.annotations Annotations added to the Temporal Jobs Service API Deployment.
    annotations: {}

    ## @param temporalJobsService.api.extraEnvVariables Extra environment variables to add to Temporal Jobs Service API pods
    ## Example:
    ## extraEnvVariables:
    ##   - name: FOO
    ##     value: BAR
    extraEnvVariables: []

    ## External Secrets Configuration
    ## @param temporalJobsService.api.externalSecrets.enabled Enable external secrets for the Temporal Jobs Service API Deployment
    ## @param temporalJobsService.api.externalSecrets.secrets List of external secrets to load environment variables from
    externalSecrets:
      enabled: false
      ## ["secret-1", "secret-2"]
      secrets: []

    ## @param temporalJobsService.api.podAnnotations Annotations to add to the resulting Pods of the Deployment.
    podAnnotations: {}

    ## @param temporalJobsService.api.podLabels Labels to add to the resulting Pods of the Deployment.
    podLabels: {}

    ## @param temporalJobsService.api.podSecurityContext Pod security context
    podSecurityContext: {}

    ## @param temporalJobsService.api.securityContext Security context for the container
    securityContext: {}
    #  runAsUser: 1000
    #  runAsGroup: 1000

    ## @param temporalJobsService.api.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    resources: {}
      ## To use the recommended values, uncomment the following lines and remove the curly braces after 'resources:'
      # requests:
      #   cpu: 1
      #   memory: 1Gi
      # limits:
      #   cpu: 2
      #   memory: 2Gi

    ## Temporal Jobs Service API Liveness probe configuration
    livenessProbe:
      ## @param temporalJobsService.api.livenessProbe.httpGet.path Path to hit for the liveness probe
      ## @param temporalJobsService.api.livenessProbe.httpGet.port Port to hit for the liveness probe
      httpGet:
        path: /healthcheck
        port: tj-api-http

    ## Temporal Jobs Service API Readiness probe configuration
    readinessProbe:
      ## @param temporalJobsService.api.readinessProbe.httpGet.path Path to hit for the readiness probe
      ## @param temporalJobsService.api.readinessProbe.httpGet.port Port to hit for the readiness probe
      httpGet:
        path: /healthcheck
        port: tj-api-http

    ## Temporal Jobs Service API Startup probe configuration
    startupProbe:
      ## @param temporalJobsService.api.startupProbe.httpGet.path Path to hit for the startup probe
      ## @param temporalJobsService.api.startupProbe.httpGet.port Port to hit for the startup probe
      httpGet:
        path: /healthcheck
        port: tj-api-http

    ## Temporal Jobs Service API HorizontalPodAutoScaler configuration
    ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
    ## The HPA scales by the target CPU utilization percentage by default
    ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
    autoscaling:
      ## @param temporalJobsService.api.autoscaling.enabled Enable autoscaling for the Temporal Jobs Service API Deployment
      ## @param temporalJobsService.api.autoscaling.minReplicas Minimum number of replicas for the Temporal Jobs Service API Deployment
      ## @param temporalJobsService.api.autoscaling.maxReplicas Maximum number of replicas for the Temporal Jobs Service API Deployment
      ## @param temporalJobsService.api.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the Temporal Jobs Service API Deployment
      enabled: false
      minReplicas: 1
      maxReplicas: 4
      targetCPUUtilizationPercentage: 80
      # targetMemoryUtilizationPercentage: 80

    ## @param temporalJobsService.api.volumes List of volumes that can be mounted by containers belonging to the pod
    ## @param temporalJobsService.api.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
    volumes: []
    volumeMounts: []

    ## @param temporalJobsService.api.nodeSelector Node labels for pod assignment
    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    nodeSelector: {}

    ## @param temporalJobsService.api.tolerations Taints to tolerate on node assignment:
    ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []

    ## @param temporalJobsService.api.affinity Pod scheduling constraints
    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
    affinity: {}

  jobsServiceWorker:
    ## @param temporalJobsService.jobsServiceWorker.name Name suffix of the Temporal Jobs Service Worker related resources
    name: temporal-jobs-service

    ## @param temporalJobsService.jobsServiceWorker.command Command to run in the container
    command: ["temporal_jobs_service"]

    ## @param temporalJobsService.jobsServiceWorker.replicas Number of replicas of Temporal Jobs Service Worker Deployment
    replicas: 1

    ## Temporal Jobs Service Worker configuration
    ## @param temporalJobsService.jobsServiceWorker.config.logLevel Log level for the Temporal Jobs Service Worker
    config:
      logLevel: "info"

    ## Temporal Jobs Service Worker Image information
    ## @param temporalJobsService.jobsServiceWorker.image.registry Temporal Jobs Service Worker Image registry
    ## @param temporalJobsService.jobsServiceWorker.image.repository Temporal Jobs Service Worker Image repository
    ## @param temporalJobsService.jobsServiceWorker.image.tag Temporal Jobs Service Worker Image tag
    ## @param temporalJobsService.jobsServiceWorker.image.pullPolicy Temporal Jobs Service Worker Image pull policy
    image:
      registry: docker.io
      repository: llamaindex/llamacloud-backend
      tag: 0.5.7
      pullPolicy: IfNotPresent

    ## ServiceAccount configuration
    ## @param temporalJobsService.jobsServiceWorker.serviceAccount.create Whether or not to create a new service account
    ## @param temporalJobsService.jobsServiceWorker.serviceAccount.name Name of the service account
    ## @param temporalJobsService.jobsServiceWorker.serviceAccount.labels Labels to add to the service account
    ## @param temporalJobsService.jobsServiceWorker.serviceAccount.annotations Annotations to add to the service account
    ## @param temporalJobsService.jobsServiceWorker.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
    serviceAccount:
      create: true
      name: ""
      labels: {}
      annotations: {}
      automountServiceAccountToken: true

    ## @param temporalJobsService.jobsServiceWorker.containerPort Port to expose on the Temporal Jobs Service Worker container
    ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
    containerPort: 8000

    ## Temporal Jobs Service Worker Service information
    ## @param temporalJobsService.jobsServiceWorker.service.type Temporal Jobs Service Worker Service type
    ## @param temporalJobsService.jobsServiceWorker.service.port Temporal Jobs Service Worker Service port
    ## @param temporalJobsService.jobsServiceWorker.service.labels Labels to add to the service
    ## @param temporalJobsService.jobsServiceWorker.service.annotations Annotations to add to the service
    service:
      type: ClusterIP
      port: 8000
      labels: {}
      annotations: {}

    ## @param temporalJobsService.jobsServiceWorker.labels Labels added to the Temporal Jobs Service Worker Deployment.
    labels: {}
    ## @param temporalJobsService.jobsServiceWorker.annotations Annotations added to the Temporal Jobs Service Worker Deployment.
    annotations: {}

    ## @param temporalJobsService.jobsServiceWorker.extraEnvVariables Extra environment variables to add to Temporal Jobs Service Worker pods
    ## Example:
    ## extraEnvVariables:
    ##   - name: FOO
    ##     value: BAR
    extraEnvVariables: []

    ## External Secrets Configuration
    ## @param temporalJobsService.jobsServiceWorker.externalSecrets.enabled Enable external secrets for the Temporal Jobs Service Worker Deployment
    ## @param temporalJobsService.jobsServiceWorker.externalSecrets.secrets List of external secrets to load environment variables from
    externalSecrets:
      enabled: false
      ## ["secret-1", "secret-2"]
      secrets: []

    ## @param temporalJobsService.jobsServiceWorker.podAnnotations Annotations to add to the resulting Pods of the Deployment.
    podAnnotations: {}

    ## @param temporalJobsService.jobsServiceWorker.podLabels Labels to add to the resulting Pods of the Deployment.
    podLabels: {}

    ## @param temporalJobsService.jobsServiceWorker.podSecurityContext Pod security context
    podSecurityContext: {}

    ## @param temporalJobsService.jobsServiceWorker.securityContext Security context for the container
    securityContext: {}
    #  runAsUser: 1000
    #  runAsGroup: 1000

    ## @param temporalJobsService.jobsServiceWorker.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    resources: {}
      ## To use the recommended values, uncomment the following lines and remove the curly braces after 'resources:'
      # requests:
      #   cpu: 1
      #   memory: 1Gi
      # limits:
      #   cpu: 2
      #   memory: 2Gi

    ## Temporal Jobs Service Worker Liveness probe configuration
    livenessProbe:
      ## @param temporalJobsService.jobsServiceWorker.livenessProbe.httpGet.path Path to hit for the liveness probe
      ## @param temporalJobsService.jobsServiceWorker.livenessProbe.httpGet.port Port to hit for the liveness probe
      ## @param temporalJobsService.jobsServiceWorker.livenessProbe.periodSeconds How often (in seconds) to perform the probe
      ## @param temporalJobsService.jobsServiceWorker.livenessProbe.timeoutSeconds Number of seconds after which the probe times out
      ## @param temporalJobsService.jobsServiceWorker.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
      httpGet:
        path: /healthcheck
        port: 8000
      periodSeconds: 30
      timeoutSeconds: 10
      failureThreshold: 5

    ## Temporal Jobs Service Worker HorizontalPodAutoScaler configuration
    ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
    ## The HPA scales by the target CPU utilization percentage by default
    ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
    autoscaling:
      ## @param temporalJobsService.jobsServiceWorker.autoscaling.enabled Enable autoscaling for the Temporal Jobs Service Worker Deployment
      ## @param temporalJobsService.jobsServiceWorker.autoscaling.minReplicas Minimum number of replicas for the Temporal Jobs Service Worker Deployment
      ## @param temporalJobsService.jobsServiceWorker.autoscaling.maxReplicas Maximum number of replicas for the Temporal Jobs Service Worker Deployment
      ## @param temporalJobsService.jobsServiceWorker.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the Temporal Jobs Service Worker Deployment
      enabled: false
      minReplicas: 1
      maxReplicas: 4
      targetCPUUtilizationPercentage: 80
      # targetMemoryUtilizationPercentage: 80

    ## KEDA based autoscaling for Temporal Jobs Service Worker Deployment
    ## Ref: https://keda.sh
    ## Note: .Values.temporalJobsService.jobsServiceWorker.autoscaling.enabled must be false for KEDA to be enabled
    keda:
      ## @param temporalJobsService.jobsServiceWorker.keda.enabled Enable KEDA for the Temporal Jobs Service Worker Deployment
      enabled: false
      ## @param temporalJobsService.jobsServiceWorker.keda.additionalAnnotations Additional annotations for the KEDA
      additionalAnnotations: {}
      ## @param temporalJobsService.jobsServiceWorker.keda.additionalLabels Additional labels for the KEDA
      additionalLabels: {}
      ## @param temporalJobsService.jobsServiceWorker.keda.pollingInterval Polling interval for the KEDA
      pollingInterval: 15
      ## @param temporalJobsService.jobsServiceWorker.keda.cooldownPeriod Cooldown period for the KEDA
      cooldownPeriod: 120
      ## @param temporalJobsService.jobsServiceWorker.keda.minReplicaCount Minimum number of replicas for the KEDA
      minReplicaCount: 1
      ## @param temporalJobsService.jobsServiceWorker.keda.maxReplicaCount Maximum number of replicas for the KEDA
      maxReplicaCount: 10
      ## @param temporalJobsService.jobsServiceWorker.keda.initialCooldownPeriod Initial cooldown period for the KEDA
      initialCooldownPeriod: 0
      ## @param temporalJobsService.jobsServiceWorker.keda.fallback Fallback configuration for the KEDA
      ## Ref: https://keda.sh/docs/2.10/concepts/scaling-deployments/#fallback
      fallback: {}
        # failureThreshold: 3
        # replicas: 6
      ## @param temporalJobsService.jobsServiceWorker.keda.advanced Advanced configuration for the KEDA
      ## Ref: https://keda.sh/docs/2.10/concepts/scaling-deployments/#advanced
      advanced: {}
      ## temporalJobsService.jobsServiceWorker.keda.triggers: Triggers for the KEDA
      ## Default trigger uses Temporal scaler: https://keda.sh/docs/2.17/scalers/temporal/
      ## NOTE: You must update the taskQueue value for your specific worker queue name
      ## @param temporalJobsService.jobsServiceWorker.keda.triggers[0].type Trigger type (temporal)
      ## @param temporalJobsService.jobsServiceWorker.keda.triggers[0].metadata.taskQueue Temporal task queue name for this worker
      ## @param temporalJobsService.jobsServiceWorker.keda.triggers[0].metadata.targetQueueSize Target queue size for scaling
      ## @param temporalJobsService.jobsServiceWorker.keda.triggers[0].metadata.activationTargetQueueSize Activation target queue size for scaling
      ## @param temporalJobsService.jobsServiceWorker.keda.triggers[0].metadata.endpoint Temporal endpoint URL
      triggers:
        - type: temporal
          metadata:
            taskQueue: "job-service"
            ## Target queue size (from k8sctl defaults: BaseScaledObjectConfig.target_queue_size)
            targetQueueSize: "2"
            ## Activation target queue size (from k8sctl defaults: BaseScaledObjectConfig.activation_target_queue_size)
            activationTargetQueueSize: "0"
            ## Temporal namespace (optional) - the Temporal workflow namespace, defaults to "default" if not set
            ## Uncomment to specify a custom Temporal namespace:
            # namespace: "default"
            ## Temporal endpoint - dynamically resolves to internal or external temporal service
            endpoint: "{{ printf \"%s:%s\" (include \"temporal.host\" .) (include \"temporal.port\" .) }}"

    ## Prometheus Metrics Configuration
    metrics:
      ## @param temporalJobsService.jobsServiceWorker.metrics.enabled Enable exposing metrics to be gathered by Prometheus
      enabled: false
      serviceMonitor:
        ## @param temporalJobsService.jobsServiceWorker.metrics.serviceMonitor.enabled Create ServiceMonitor resource for scraping metrics using PrometheusOperator
        enabled: false
        ## @param temporalJobsService.jobsServiceWorker.metrics.serviceMonitor.selector ServiceMonitor selector for the labels to be scraped
        selector: {}
        ## @param temporalJobsService.jobsServiceWorker.metrics.serviceMonitor.additionalLabels Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with
        additionalLabels: {}
        ## @param temporalJobsService.jobsServiceWorker.metrics.serviceMonitor.annotations ServiceMonitor annotations
        annotations: {}
        ## @param temporalJobsService.jobsServiceWorker.metrics.serviceMonitor.interval Interval at which metrics should be scraped
        interval: 30s
        ## @param temporalJobsService.jobsServiceWorker.metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended
        scrapeTimeout: 15s
        ## @param temporalJobsService.jobsServiceWorker.metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
        relabelings: []
        ## @param temporalJobsService.jobsServiceWorker.metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
        metricRelabelings: []
        ## @param temporalJobsService.jobsServiceWorker.metrics.serviceMonitor.scheme HTTP scheme to use for scraping
        scheme: http
        ## @param temporalJobsService.jobsServiceWorker.metrics.serviceMonitor.tlsConfig TLS configuration to use when scraping the endpoint
        tlsConfig: {}

    ## @param temporalJobsService.jobsServiceWorker.volumes List of volumes that can be mounted by containers belonging to the pod
    ## @param temporalJobsService.jobsServiceWorker.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
    volumes: []
    volumeMounts: []

    ## @param temporalJobsService.jobsServiceWorker.nodeSelector Node labels for pod assignment
    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    nodeSelector: {}

    ## @param temporalJobsService.jobsServiceWorker.tolerations Taints to tolerate on node assignment:
    ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []

    ## @param temporalJobsService.jobsServiceWorker.affinity Pod scheduling constraints
    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
    affinity: {}

  jobsExtractWorker:
    ## @param temporalJobsService.jobsExtractWorker.name Name suffix of the Temporal Extract Worker related resources
    name: temporal-extract

    ## @param temporalJobsService.jobsExtractWorker.command Command to run in the container
    command: ["temporal_extract"]

    ## @param temporalJobsService.jobsExtractWorker.replicas Number of replicas of Temporal Extract Worker Deployment
    replicas: 1

    ## Temporal Extract Worker configuration
    ## @param temporalJobsService.jobsExtractWorker.config.logLevel Log level for the Temporal Extract Worker
    config:
      logLevel: "info"

    ## Temporal Extract Worker Image information
    ## @param temporalJobsService.jobsExtractWorker.image.registry Temporal Extract Worker Image registry
    ## @param temporalJobsService.jobsExtractWorker.image.repository Temporal Extract Worker Image repository
    ## @param temporalJobsService.jobsExtractWorker.image.tag Temporal Extract Worker Image tag
    ## @param temporalJobsService.jobsExtractWorker.image.pullPolicy Temporal Extract Worker Image pull policy
    image:
      registry: docker.io
      repository: llamaindex/llamacloud-backend
      tag: 0.5.7
      pullPolicy: IfNotPresent

    ## ServiceAccount configuration
    ## @param temporalJobsService.jobsExtractWorker.serviceAccount.create Whether or not to create a new service account
    ## @param temporalJobsService.jobsExtractWorker.serviceAccount.name Name of the service account
    ## @param temporalJobsService.jobsExtractWorker.serviceAccount.labels Labels to add to the service account
    ## @param temporalJobsService.jobsExtractWorker.serviceAccount.annotations Annotations to add to the service account
    ## @param temporalJobsService.jobsExtractWorker.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
    serviceAccount:
      create: true
      name: ""
      labels: {}
      annotations: {}
      automountServiceAccountToken: true

    ## @param temporalJobsService.jobsExtractWorker.containerPort Port to expose on the Temporal Extract Worker container
    ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
    containerPort: 8000

    ## Temporal Extract Worker Service information
    ## @param temporalJobsService.jobsExtractWorker.service.type Temporal Extract Worker Service type
    ## @param temporalJobsService.jobsExtractWorker.service.port Temporal Extract Worker Service port
    ## @param temporalJobsService.jobsExtractWorker.service.labels Labels to add to the service
    ## @param temporalJobsService.jobsExtractWorker.service.annotations Annotations to add to the service
    service:
      type: ClusterIP
      port: 8000
      labels: {}
      annotations: {}

    ## @param temporalJobsService.jobsExtractWorker.labels Labels added to the Temporal Extract Worker Deployment.
    labels: {}
    ## @param temporalJobsService.jobsExtractWorker.annotations Annotations added to the Temporal Extract Worker Deployment.
    annotations: {}

    ## @param temporalJobsService.jobsExtractWorker.extraEnvVariables Extra environment variables to add to Temporal Extract Worker pods
    ## Example:
    ## extraEnvVariables:
    ##   - name: FOO
    ##     value: BAR
    extraEnvVariables: []

    ## External Secrets Configuration
    ## @param temporalJobsService.jobsExtractWorker.externalSecrets.enabled Enable external secrets for the Temporal Extract Worker Deployment
    ## @param temporalJobsService.jobsExtractWorker.externalSecrets.secrets List of external secrets to load environment variables from
    externalSecrets:
      enabled: false
      ## ["secret-1", "secret-2"]
      secrets: []

    ## @param temporalJobsService.jobsExtractWorker.podAnnotations Annotations to add to the resulting Pods of the Deployment.
    podAnnotations: {}

    ## @param temporalJobsService.jobsExtractWorker.podLabels Labels to add to the resulting Pods of the Deployment.
    podLabels: {}

    ## @param temporalJobsService.jobsExtractWorker.podSecurityContext Pod security context
    podSecurityContext: {}

    ## @param temporalJobsService.jobsExtractWorker.securityContext Security context for the container
    securityContext: {}
    #  runAsUser: 1000
    #  runAsGroup: 1000

    ## @param temporalJobsService.jobsExtractWorker.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    resources: {}
      ## To use the recommended values, uncomment the following lines and remove the curly braces after 'resources:'
      # requests:
      #   cpu: 1
      #   memory: 1Gi
      # limits:
      #   cpu: 2
      #   memory: 2Gi

    ## Temporal Extract Worker Liveness probe configuration
    livenessProbe:
      ## @param temporalJobsService.jobsExtractWorker.livenessProbe.httpGet.path Path to hit for the liveness probe
      ## @param temporalJobsService.jobsExtractWorker.livenessProbe.httpGet.port Port to hit for the liveness probe
      ## @param temporalJobsService.jobsExtractWorker.livenessProbe.periodSeconds How often (in seconds) to perform the probe
      ## @param temporalJobsService.jobsExtractWorker.livenessProbe.timeoutSeconds Number of seconds after which the probe times out
      ## @param temporalJobsService.jobsExtractWorker.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
      httpGet:
        path: /healthcheck
        port: 8000
      periodSeconds: 30
      timeoutSeconds: 10
      failureThreshold: 5

    ## Temporal Extract Worker HorizontalPodAutoScaler configuration
    ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
    ## The HPA scales by the target CPU utilization percentage by default
    ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
    autoscaling:
      ## @param temporalJobsService.jobsExtractWorker.autoscaling.enabled Enable autoscaling for the Temporal Extract Worker Deployment
      ## @param temporalJobsService.jobsExtractWorker.autoscaling.minReplicas Minimum number of replicas for the Temporal Extract Worker Deployment
      ## @param temporalJobsService.jobsExtractWorker.autoscaling.maxReplicas Maximum number of replicas for the Temporal Extract Worker Deployment
      ## @param temporalJobsService.jobsExtractWorker.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the Temporal Extract Worker Deployment
      enabled: false
      minReplicas: 1
      maxReplicas: 4
      targetCPUUtilizationPercentage: 80
      # targetMemoryUtilizationPercentage: 80

    ## KEDA based autoscaling for Temporal Extract Worker Deployment
    ## Ref: https://keda.sh
    ## Note: .Values.temporalJobsService.jobsExtractWorker.autoscaling.enabled must be false for KEDA to be enabled
    keda:
      ## @param temporalJobsService.jobsExtractWorker.keda.enabled Enable KEDA for the Temporal Extract Worker Deployment
      enabled: false
      ## @param temporalJobsService.jobsExtractWorker.keda.additionalAnnotations Additional annotations for the KEDA
      additionalAnnotations: {}
      ## @param temporalJobsService.jobsExtractWorker.keda.additionalLabels Additional labels for the KEDA
      additionalLabels: {}
      ## @param temporalJobsService.jobsExtractWorker.keda.pollingInterval Polling interval for the KEDA
      pollingInterval: 15
      ## @param temporalJobsService.jobsExtractWorker.keda.cooldownPeriod Cooldown period for the KEDA
      cooldownPeriod: 120
      ## @param temporalJobsService.jobsExtractWorker.keda.minReplicaCount Minimum number of replicas for the KEDA
      minReplicaCount: 1
      ## @param temporalJobsService.jobsExtractWorker.keda.maxReplicaCount Maximum number of replicas for the KEDA
      maxReplicaCount: 10
      ## @param temporalJobsService.jobsExtractWorker.keda.initialCooldownPeriod Initial cooldown period for the KEDA
      initialCooldownPeriod: 0
      ## @param temporalJobsService.jobsExtractWorker.keda.fallback Fallback configuration for the KEDA
      ## Ref: https://keda.sh/docs/2.10/concepts/scaling-deployments/#fallback
      fallback: {}
        # failureThreshold: 3
        # replicas: 6
      ## @param temporalJobsService.jobsExtractWorker.keda.advanced Advanced configuration for the KEDA
      ## Ref: https://keda.sh/docs/2.10/concepts/scaling-deployments/#advanced
      advanced: {}
      ## temporalJobsService.jobsExtractWorker.keda.triggers: Triggers for the KEDA
      ## Default trigger uses Temporal scaler: https://keda.sh/docs/2.17/scalers/temporal/
      ## NOTE: You must update the taskQueue value for your specific worker queue name
      ## @param temporalJobsService.jobsExtractWorker.keda.triggers[0].type Trigger type (temporal)
      ## @param temporalJobsService.jobsExtractWorker.keda.triggers[0].metadata.taskQueue Temporal task queue name for this worker
      ## @param temporalJobsService.jobsExtractWorker.keda.triggers[0].metadata.targetQueueSize Target queue size for scaling
      ## @param temporalJobsService.jobsExtractWorker.keda.triggers[0].metadata.activationTargetQueueSize Activation target queue size for scaling
      ## @param temporalJobsService.jobsExtractWorker.keda.triggers[0].metadata.endpoint Temporal endpoint URL
      triggers:
        - type: temporal
          metadata:
            taskQueue: "extract"
            ## Target queue size (from k8sctl defaults: BaseScaledObjectConfig.target_queue_size)
            targetQueueSize: "2"
            ## Activation target queue size (from k8sctl defaults: BaseScaledObjectConfig.activation_target_queue_size)
            activationTargetQueueSize: "0"
            ## Temporal namespace (optional) - the Temporal workflow namespace, defaults to "default" if not set
            ## Uncomment to specify a custom Temporal namespace:
            # namespace: "default"
            ## Temporal endpoint - dynamically resolves to internal or external temporal service
            endpoint: "{{ printf \"%s:%s\" (include \"temporal.host\" .) (include \"temporal.port\" .) }}"

    ## Prometheus Metrics Configuration
    metrics:
      ## @param temporalJobsService.jobsExtractWorker.metrics.enabled Enable exposing metrics to be gathered by Prometheus
      enabled: false
      serviceMonitor:
        ## @param temporalJobsService.jobsExtractWorker.metrics.serviceMonitor.enabled Create ServiceMonitor resource for scraping metrics using PrometheusOperator
        enabled: false
        ## @param temporalJobsService.jobsExtractWorker.metrics.serviceMonitor.selector ServiceMonitor selector for the labels to be scraped
        selector: {}
        ## @param temporalJobsService.jobsExtractWorker.metrics.serviceMonitor.additionalLabels Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with
        additionalLabels: {}
        ## @param temporalJobsService.jobsExtractWorker.metrics.serviceMonitor.annotations ServiceMonitor annotations
        annotations: {}
        ## @param temporalJobsService.jobsExtractWorker.metrics.serviceMonitor.interval Interval at which metrics should be scraped
        interval: 30s
        ## @param temporalJobsService.jobsExtractWorker.metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended
        scrapeTimeout: 15s
        ## @param temporalJobsService.jobsExtractWorker.metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
        relabelings: []
        ## @param temporalJobsService.jobsExtractWorker.metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
        metricRelabelings: []
        ## @param temporalJobsService.jobsExtractWorker.metrics.serviceMonitor.scheme HTTP scheme to use for scraping
        scheme: http
        ## @param temporalJobsService.jobsExtractWorker.metrics.serviceMonitor.tlsConfig TLS configuration to use when scraping the endpoint
        tlsConfig: {}

    ## @param temporalJobsService.jobsExtractWorker.volumes List of volumes that can be mounted by containers belonging to the pod
    ## @param temporalJobsService.jobsExtractWorker.volumeMounts List of volumeMounts that can be mounted by containers belonging to the pod
    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
    volumes: []
    volumeMounts: []

    ## @param temporalJobsService.jobsExtractWorker.nodeSelector Node labels for pod assignment
    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    nodeSelector: {}

    ## @param temporalJobsService.jobsExtractWorker.tolerations Taints to tolerate on node assignment:
    ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []

    ## @param temporalJobsService.jobsExtractWorker.affinity Pod scheduling constraints
    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
    affinity: {}

## @section S3Proxy Configuration
s3proxy:
  ## @param s3proxy.enabled Enable s3proxy Deployment
  enabled: true

  ## @param s3proxy.name Name suffix of the s3proxy related resources
  name: s3proxy

  ## @param s3proxy.config s3proxy configuration to enable s3proxy features
  ## For reference for configuration examples, please visit the following page:
  ## - https://github.com/gaul/s3proxy/wiki/Storage-backend-examples
  ## For reference for available configuration options, see the s3proxy Dockerfile here:
  ## - https://github.com/gaul/s3proxy/blob/master/Dockerfile
  config: {}
    # Example configuration for azure blob storage:
    # S3PROXY_ENDPOINT: "http://0.0.0.0:80"
    # S3PROXY_AUTHORIZATION: "none"
    # JCLOUDS_PROVIDER: "azureblob"
    # JCLOUDS_AZUREBLOB_AUTH: "azureKey"
    # JCLOUDS_IDENTITY: "<azure-storage-account-name>"
    # JCLOUDS_CREDENTIAL: "<azure-storage-account-key>"
    # JCLOUDS_ENDPOINT: "<azure-storage-account-endpoint>"

  ## @param s3proxy.replicas Number of replicas of s3proxy Deployment
  replicas: 1

  ## Image information
  ## @param s3proxy.image.registry s3proxy Image registry
  ## @param s3proxy.image.repository s3proxy Image repository
  ## @param s3proxy.image.tag s3proxy Image tag
  ## @param s3proxy.image.pullPolicy s3proxy Image pull policy
  image:
    registry: docker.io
    repository: andrewgaul/s3proxy
    tag: sha-82e50ee
    pullPolicy: IfNotPresent

  ## s3proxy Service information
  ## @param s3proxy.service.type s3proxy Service type
  ## @param s3proxy.service.port s3proxy Service port
  service:
    type: ClusterIP
    port: 80

  ## ServiceAccount configuration
  ## @param s3proxy.serviceAccount.create Whether or not to create a new service account
  ## @param s3proxy.serviceAccount.name Name of the service account
  ## @param s3proxy.serviceAccount.labels Labels to add to the service account
  ## @param s3proxy.serviceAccount.annotations Annotations to add to the service account
  ## @param s3proxy.serviceAccount.automountServiceAccountToken Whether or not to automount the service account token
  serviceAccount:
    create: true
    name: ""
    labels: {}
    annotations: {}
    automountServiceAccountToken: true

  ## @param s3proxy.containerPort Port to expose on the s3proxy container
  ## Ref: https://kubernetes.io/docs/tutorials/services/connect-applications-service/#exposing-pods-to-the-cluster
  containerPort: 80

  ## @param s3proxy.labels Labels added to the s3proxy Deployment.
  labels: {}
  ## @param s3proxy.annotations Annotations added to the s3proxy Deployment.
  annotations: {}

  ## @param s3proxy.extraEnvVariables Extra environment variables to add to s3proxy pods
  ## Example:
  ## extraEnvVariables:
  ##   - name: FOO
  ##     value: BAR
  ##   - name: OPENAI_API_KEY
  ##     valueFrom:
  ##       secretKeyRef:
  ##         name: openai-api-secret
  ##         key: openai-api-key
  extraEnvVariables: []

  ## @param s3proxy.envFromSecretName Name of the secret to use for environment variables
  ## Loads environment variables from a secret object
  envFromSecretName: ""
  ## @param s3proxy.envFromConfigMapName Name of the config map to use for environment variables
  ## Loads environment variables from a ConfigMap object
  envFromConfigMapName: ""

  ## @param s3proxy.podAnnotations Annotations to add to the resulting Pods of the Deployment.
  podAnnotations: {}

  ## @param s3proxy.podLabels Labels to add to the resulting Pods of the Deployment.
  podLabels: {}

  ## @param s3proxy.podSecurityContext Pod security context
  podSecurityContext: {}

  ## @param s3proxy.securityContext Security context for the container
  securityContext: {}
  #  runAsUser: 1000
  #  runAsGroup: 1000

  ## @param s3proxy.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
    ## To use the recommended values, uncomment the following lines and remove the curly braces after 'resources:'
    # requests:
    #   cpu: 1
    #   memory: 1Gi
    # limits:
    #   cpu: 4
    #   memory: 2Gi

  ## s3Proxy HorizontalPodAutoScaler configuration
  ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  ## The HPA scales by the target CPU utilization percentage by default
  ## Uncomment the targetMemoryUtilizationPercentage line below to scale by memory utilization
  autoscaling:
    ## @param s3proxy.autoscaling.enabled Enable autoscaling for the s3proxy Deployment
    ## @param s3proxy.autoscaling.minReplicas Minimum number of replicas for the s3proxy Deployment
    ## @param s3proxy.autoscaling.maxReplicas Maximum number of replicas for the s3proxy Deployment
    ## @param s3proxy.autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage for the s3proxy Deployment
    enabled: false
    minReplicas: 1
    maxReplicas: 4
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  ## @param s3proxy.nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}

  ## @param s3proxy.tolerations Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## @param s3proxy.affinity Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}


################
### DEPENDENCIES
################

## @section Dependencies Configuration
postgresql:
  ## @param postgresql.enabled Enable PostgreSQL
  enabled: true

  image:
    ## @param postgresql.image.registry PostgreSQL Image registry
    ## @param postgresql.image.repository PostgreSQL Image repository
    registry: docker.io
    repository: bitnamilegacy/postgresql

  # Postgres Auth
  auth:
    ## @param postgresql.auth.enabled Enable PostgreSQL Auth
    ## @param postgresql.auth.database Database name
    ## @param postgresql.auth.username Username
    enabled: true
    database: llamacloud
    username: llamacloud

  ## Ref: https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml#L481
  primary:
    ## @param postgresql.primary.resources.requests.cpu CPU requests
    ## @param postgresql.primary.resources.requests.memory Memory requests
    ## @param postgresql.primary.resources.limits.cpu CPU limits
    ## @param postgresql.primary.resources.limits.memory Memory limits
    resources:
      requests:
        cpu: 1
        memory: 1Gi
      limits:
        cpu: 2
        memory: 2Gi

mongodb:
  ## @param mongodb.enabled Enable MongoDB
  enabled: true

  image:
    ## @param mongodb.image.registry MongoDB Image registry
    ## @param mongodb.image.repository MongoDB Image repository
    registry: docker.io
    repository: bitnamilegacy/mongodb

  # MongoDB Auth
  auth:
    ## @param mongodb.auth.enabled Enable MongoDB Auth
    ## @param mongodb.auth.rootUser Root user name
    enabled: true

    rootUser: root

redis:
  ## @param redis.enabled Enable Redis
  enabled: true

  image:
    ## @param redis.image.registry Redis Image registry
    ## @param redis.image.repository Redis Image repository
    registry: docker.io
    repository: bitnamilegacy/redis

  ## DO NOT ENABLE REDIS AUTH FOR NOW
  auth:
    ## @param redis.auth.enabled Enable Redis Auth (DO NOT ENABLE FOR NOW)
    enabled: false

rabbitmq:
  ## @param rabbitmq.enabled Enable RabbitMQ
  enabled: true

  image:
    ## @param rabbitmq.image.registry RabbitMQ Image registry
    ## @param rabbitmq.image.repository RabbitMQ Image repository
    registry: docker.io
    repository: bitnamilegacy/rabbitmq

temporal:
  ## @param temporal.enabled Enable Temporal
  ## IMPORTANT: When enabling Temporal, you must configure the database connection.
  ## If using the postgresql subchart (postgresql.enabled=true):
  ##   Set host to: "<your-release-name>-postgresql"
  ##   Set existingSecret to: "<your-release-name>-postgresql"
  ##   Set user to match postgresql.auth.username (default: "llamacloud")
  ## Example for release name "my-release":
  ##   temporal.server.config.persistence.default.sql.host: "my-release-postgresql"
  ##   temporal.server.config.persistence.default.sql.existingSecret: "my-release-postgresql"
  enabled: false

  cassandra:
    ## @param temporal.cassandra.enabled Enable Cassandra
    enabled: false

  prometheus:
    ## @param temporal.prometheus.enabled Enable Prometheus
    enabled: false

  grafana:
    ## @param temporal.grafana.enabled Enable Grafana
    enabled: false

  elasticsearch:
    ## @param temporal.elasticsearch.enabled Enable Elasticsearch
    enabled: false

  server:
    config:
      namespaces:
        ## @param temporal.server.config.namespaces.create Enable this to create namespaces
        create: true
      persistence:
        default:
          ## @param temporal.server.config.persistence.default.driver Persistence driver (should be "sql")
          driver: "sql"
          sql:
            ## @param temporal.server.config.persistence.default.sql.driver SQL driver for default persistence (e.g., "postgres12")
            driver: "postgres12"
            ## @param temporal.server.config.persistence.default.sql.host SQL host for default persistence. REQUIRED: Set to "<release-name>-postgresql" when using postgresql subchart
            host: ""
            ## @param temporal.server.config.persistence.default.sql.port SQL port for default persistence
            port: 5432
            ## @param temporal.server.config.persistence.default.sql.database SQL database name for default persistence
            database: "temporal"
            ## @param temporal.server.config.persistence.default.sql.user SQL user for default persistence (should match postgresql.auth.username)
            user: "llamacloud"
            ## @param temporal.server.config.persistence.default.sql.password SQL password for default persistence
            password: ""
            ## @param temporal.server.config.persistence.default.sql.existingSecret Existing secret for default SQL persistence. REQUIRED: Set to "<release-name>-postgresql" when using postgresql subchart
            existingSecret: ""
            ## @param temporal.server.config.persistence.default.sql.secretName Secret name for default SQL persistence (if applicable)
            secretName: ""
            ## @param temporal.server.config.persistence.default.sql.maxConns Max SQL connections for default persistence
            maxConns: 20
            ## @param temporal.server.config.persistence.default.sql.maxIdleConns Max idle SQL connections for default persistence
            maxIdleConns: 20
            ## @param temporal.server.config.persistence.default.sql.maxConnLifetime Lifetime for SQL connections (default persistence)
            maxConnLifetime: "1h"

        visibility:
          ## @param temporal.server.config.persistence.visibility.driver Persistence driver for visibility (should be "sql")
          driver: "sql"
          sql:
            ## @param temporal.server.config.persistence.visibility.sql.driver SQL driver for visibility persistence (e.g., "postgres12")
            driver: "postgres12"
            ## @param temporal.server.config.persistence.visibility.sql.host SQL host for visibility persistence. REQUIRED: Set to "<release-name>-postgresql" when using postgresql subchart
            host: ""
            ## @param temporal.server.config.persistence.visibility.sql.port SQL port for visibility persistence
            port: 5432
            ## @param temporal.server.config.persistence.visibility.sql.database SQL database name for visibility persistence
            database: "temporal_visibility"
            ## @param temporal.server.config.persistence.visibility.sql.user SQL user for visibility persistence (should match postgresql.auth.username)
            user: "llamacloud"
            ## @param temporal.server.config.persistence.visibility.sql.password SQL password for visibility persistence
            password: ""
            ## @param temporal.server.config.persistence.visibility.sql.existingSecret Existing secret for visibility SQL persistence. REQUIRED: Set to "<release-name>-postgresql" when using postgresql subchart
            existingSecret: ""
            ## @param temporal.server.config.persistence.visibility.sql.secretName Secret name for visibility SQL persistence (if applicable)
            secretName: ""
            ## @param temporal.server.config.persistence.visibility.sql.maxConns Max SQL connections for visibility persistence
            maxConns: 20
            ## @param temporal.server.config.persistence.visibility.sql.maxIdleConns Max idle SQL connections for visibility persistence
            maxIdleConns: 20
            ## @param temporal.server.config.persistence.visibility.sql.maxConnLifetime Lifetime for SQL connections (visibility persistence)
            maxConnLifetime: "1h"

## Temporal Server Configuration
## Configuration for temporal server components and setup jobs
temporalServer:
  ## Search Attributes Job
  ## @param temporalServer.searchAttributesJob.enabled Enable the search attributes job
  ## @param temporalServer.searchAttributesJob.backoffLimit Number of retries before considering job as failed
  searchAttributesJob:
    ## @param temporalServer.searchAttributesJob.enabled Enable the search attributes job
    ## @param temporalServer.searchAttributesJob.backoffLimit Number of retries before considering job as failed
    enabled: true
    backoffLimit: 3

    ## @param temporalServer.searchAttributesJob.image.repository Image repository for temporal admin tools
    ## @param temporalServer.searchAttributesJob.image.tag Image tag for temporal admin tools
    ## @param temporalServer.searchAttributesJob.image.pullPolicy Image pull policy
    image:
      ## @param temporalServer.searchAttributesJob.image.repository Image repository for temporal admin tools
      ## @param temporalServer.searchAttributesJob.image.tag Image tag for temporal admin tools
      ## @param temporalServer.searchAttributesJob.image.pullPolicy Image pull policy
      repository: temporalio/admin-tools
      tag: "1.29"
      pullPolicy: IfNotPresent

    ## temporalServer.searchAttributesJob.attributes: Search attributes to be created in Temporal
    ## Each attribute should have a name and type (Text, Keyword, Int, Double, Bool, Datetime, KeywordList)
    ## @param temporalServer.searchAttributesJob.attributes[0].name Name of the first search attribute
    ## @param temporalServer.searchAttributesJob.attributes[0].type Type of the first search attribute (Text, Keyword, Int, Double, Bool, Datetime, KeywordList)
    ## @param temporalServer.searchAttributesJob.attributes[1].name Name of the second search attribute
    ## @param temporalServer.searchAttributesJob.attributes[1].type Type of the second search attribute (Text, Keyword, Int, Double, Bool, Datetime, KeywordList)
    attributes:
      - name: Project
        type: Keyword
      - name: Organization
        type: Keyword
